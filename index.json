[{"authors":["admin"],"categories":null,"content":"A marathon runner, diver, and part-time programmer, now lives and works in Manila, used to live in BKK, Kyoto, Osaka, Zurich, Geneva, and Hanoi.\nPlaces like home: Ecopark (VN), Katsura (JP), Geneva (CH), and Zurich (CH)\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"A marathon runner, diver, and part-time programmer, now lives and works in Manila, used to live in BKK, Kyoto, Osaka, Zurich, Geneva, and Hanoi.\nPlaces like home: Ecopark (VN), Katsura (JP), Geneva (CH), and Zurich (CH)","tags":null,"title":"Nam Le","type":"authors"},{"authors":["Nam Le"],"categories":[],"content":"Rationale Technical Due Diligence (TDD) is an engineering management process for evaluation and assessment on operation and condition of assets of engineering systems (e.g. buildings, industrial plants, factories). TDD can be considered as a simple application of the Asset Management Framework.\n Buy/Acquire the property Execute an intervention strategy/program for the targeted engineering systems at a set of points in time  TDD is usually done in a quick fashion by a team of a few senior engineers and specialists who understand the targeted engineering systems. The team will normally visit the site/office to perform visual inspection and desktop study on historical records (e.g. as-building drawings, O\u0026amp;M manual, corrective and intervention intervention logbooks). In some case, the work might also involve physical testings for a certain level of audit.\na TDD work is carried out by implementing a methodology involved assessment on\n Functional Technical Quality Operational  The figure below gives a quick snapshot on how to deliver the work. In many cases, Clients might extend a TDD service to include legal and commercial aspects.\nExample In an effort to automate the TDD process in my company (Arcadis), I used R to code real examples for a few number of Clients (including a Global Investment Captical Firm and a Japanese company with its main business in railway but wanna to expand into reestate business in the Philippines)\nThree major projects I worked for these two Clients involves\n TDD and EDD for Hanjin Shipyard in Subic. This shipyard is one of the biggest one in the world. The owner Hanjin has been in bankrupcy since 2018. Till the early 2020, it has been closed, resulting the lost of employment for about 25,000 workers. This job was for a Capital Investment Firm. TDD and asset management study for a 30 storey building in Bonifacio Global City (BGC) (a Japanese railway firm) TDD and asset management for estate water facilities for Manila Water  Exammple in below link is a fictive one with values being generated randomely for demonstration purpose. However, the structure of data is of standard one for automation of TDD process.\nLink to full article on example and R code for a Technical Due Diligence Study\nKindly leave your comments in this page after reading the article. Thanks!\nReading Materials Some good articles on TDD are\n RICS Best Practice \u0026amp; Guidance Note for Technical Due Diligence of Commercial, Industrial \u0026amp; Residential Property in Continental Europe RICs Technical due diligence of commercial property  ","date":1581123451,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581123451,"objectID":"c1d590636d98c6091cda3e6e3ab29fee","permalink":"/post/tdd/","publishdate":"2020-02-08T08:57:31+08:00","relpermalink":"/post/tdd/","section":"post","summary":"Technical Due Diligence (TDD) is an engineering management process for evaluation and assessment on operation and condition of assets of engineering systems (e.g. buildings, industrial plants, factories). TDD can be considered as a simple application of the Asset Management Framework.","tags":["TDD","Asset Management"],"title":"Technical Due Diligence (TDD)","type":"post"},{"authors":[],"categories":[],"content":"This is a poem written by Bui Van Thinh, a senior guy in my class. During the period from 2002 to 2004, we were Master students at the Asian Institute of Technology (AIT) in BKK, Thailand. Many of us were poor in English but we were trying our best to learn the language from other international friends. The English of the peom contains some grammatical errors, however, the content and the meaning of the poem are more important, telling us a good memory of the past.\n It’s the time to say goodbye to you\nThe moments at AIT, I will never forget\nOur class is friendly with so many languages\nWe are all for knowledge achievement.\n  Professor Ogun. is so kind in the risk management\nExplain the difficult meanings in legal concepts\nThe more reading what saying in the FIDIC\nThe less we believe in our English skills.\n  Dr. Chotchai is a gentleman in Management field\nLike a politician being teacher in the class\nWhat he’s teaching on the construction project\nHave led more students to project managers.\n  Dr. Hadi is the youngest of the three\nBeing both teacher and friend in the class\nHis simulation, 3D and 4D knowledges\nHave attracted students as playing games.\n  All the knowledge we have learned\nBeing so useful for our future works\nBeing a vehicle with strong driving force\nOur future is therefore brightly.\n  I express my thanks to the faculties\nand to all school members and the staffs\nMy thanks are addressing to all the class\nNever could I forget these moments.\n   AIT, 26 March 2004, Bui Van Thinh\n  ","date":1575089313,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575089313,"objectID":"774650b83161e1531a89da305180594b","permalink":"/post/ait-buivanthinh-themoment/","publishdate":"2019-11-30T12:48:33+08:00","relpermalink":"/post/ait-buivanthinh-themoment/","section":"post","summary":"This is a poem written by Bui Van Thinh, a senior guy in my class. During the period from 2002 to 2004, we were Master students at the Asian Institute of Technology (AIT) in BKK, Thailand. Many of us were poor in English but we were trying our best to learn the language from other international friends. The English of the peom contains some grammatical errors, however, the content and the meaning of the poem are more important, telling us a good memory of the past.","tags":[],"title":"The moment at the AIT","type":"post"},{"authors":[],"categories":[],"content":"What is Polynomial Regression? and when to use it There is a good set of internet articles that well describe the Math behind the Polynomial regression. Some of good references can be with the following links\nhttps://en.wikipedia.org/wiki/Local_regression\nhttps://onlinelibrary.wiley.com/doi/10.1002/9781118596289.ch4\nWe use polynomial regression in cases when observing histogram, scatter plot, or distribution of data that are not well distributed or clustered into two or more than two groups as shown in below figure.\nThe plot show that the data points are scattered and seem like the pattern of them follows a sin function. We cannot use linear regression in such as case as linear regression is a straight line function.\nData Data is from the paper https://onlinelibrary.wiley.com/doi/10.1002/9781118596289.ch4 that includes 272 rows of data on the duration of eruption and waiting time until the next eruption of the Old Faithful volcano.\nOldFaithful[1:10,]\rTimeEruption TimeWaiting\r1 3.600 79\r2 1.800 54\r3 3.333 74\r4 2.283 62\r5 4.533 85\r6 2.883 55\r7 4.700 88\r8 3.600 85\r9 1.950 51\r10 4.350 85\rCode github link\nlibrary(locfit)\r## first we read in the data\r ## first we read in the data\r #OldFaithful \u0026lt;- read.csv(\u0026#34;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/OldFaithful.csv\u0026#34;)\r OldFaithful \u0026lt;- read.csv(\u0026#34;OldFaithful.csv\u0026#34;)\rOldFaithful[1:10,]\r## density histograms and smoothed density histograms\r ## time of eruption\r plot.new()\r#par(mfrow=c(1,1))\r par(mar=c(4,4,1,1)+0.1,mfrow=c(1,2),bg=\u0026#34;white\u0026#34;,cex = 1, cex.main = 1)\rhist(OldFaithful$TimeEruption,freq=FALSE)\r#use locfit https://www.rdocumentation.org/packages/locfit/versions/19980714-2/topics/locfit\r fit1 \u0026lt;- locfit(~lp(TimeEruption),data=OldFaithful)\r#lp function https://www.rdocumentation.org/packages/lpSolve/versions/5.6.13.1/topics/lp\r #lp is a local polynomial model term for Locfit models.\r #https://www.rdocumentation.org/packages/locfit/versions/1.5-9.1/topics/lp\r plot(fit1)\rdev.copy(png,\u0026#39;oldfaithful_timeeruption01.png\u0026#39;,width = 800, height = 400)\rdev.off()\r## waiting time to next eruption\r hist(OldFaithful$TimeWaiting,freq=FALSE)\rfit2 \u0026lt;- locfit(~lp(TimeWaiting),data=OldFaithful)\rplot(fit2)\rdev.copy(png,\u0026#39;oldfaithful_TimeWaiting01.png\u0026#39;,width = 800, height = 400)\rdev.off()\r#------------------------------\r ## experiment with different smoothing constants\r fit3 \u0026lt;- locfit(~lp(TimeWaiting,nn=0.9,deg=2),data=OldFaithful) #nn is the nearest neighbour component, and deg is the degree of polynomial. default value of nn is 0.6 and deg is 2.\r plot(fit3)\rfit4 \u0026lt;- locfit(~lp(TimeWaiting,nn=0.3,deg=2),data=OldFaithful)\rplot(fit4)\rdev.copy(png,\u0026#39;oldfaithful_TimeWaiting02.png\u0026#39;,width = 800, height = 400)\rdev.off()\r## cross-validation of smoothing constant\r ## for waiting time to next eruption\r alpha\u0026lt;-seq(0.20,1,by=0.01)\rn1=length(alpha)\rg=matrix(nrow=n1,ncol=4)\rfor (k in 1:length(alpha)) {\rg[k,]\u0026lt;-gcv(~lp(TimeWaiting,nn=alpha[k]),data=OldFaithful)\r}\rg\r#gcv is used to estimate the penalty coefficient from the generalized cross-validation criteria. https://www.rdocumentation.org/packages/SpatialExtremes/versions/2.0-7/topics/gcv\r plot(g[,4]~g[,3],ylab=\u0026#34;GCV\u0026#34;,xlab=\u0026#34;degrees of freedom\u0026#34;)\r#the minimum point of the curve indicate the best value of nn. In this case, we can find the minimum value point.\r which.min(g[,4])\r#This indicate\r nn=alpha[which.min(g[,4])] #this is the value of the minimum nn.\r fit5 \u0026lt;- locfit(~lp(TimeWaiting,nn=nn,deg=2),data=OldFaithful)\rplot(fit5)\rdev.copy(png,\u0026#39;oldfaithful_TimeWaiting03.png\u0026#39;,width = 800, height = 400)\rdev.off()\r#------------------------\r ## local polynomial regression of TimeEruption on TimeWaiting\r plot(TimeWaiting~TimeEruption,data=OldFaithful)\r# standard regression fit\r fitreg=lm(TimeWaiting~TimeEruption,data=OldFaithful)\rplot(TimeWaiting~TimeEruption,data=OldFaithful)\rabline(fitreg)\rdev.copy(png,\u0026#39;oldfaithful_TimeWaitingvseruption01.png\u0026#39;,width = 800, height = 400)\rdev.off()\r#-----------------------------------\r # fit with nearest neighbor bandwidth\r plot.new()\r#par(mfrow=c(1,1))\r par(mar=c(4,4,1,1)+0.1,mfrow=c(2,2),bg=\u0026#34;white\u0026#34;,cex = 1, cex.main = 1)\rfit6 \u0026lt;- locfit(TimeWaiting~lp(TimeEruption),data=OldFaithful)\rplot(fit6)\rfit7 \u0026lt;- locfit(TimeWaiting~lp(TimeEruption,deg=1),data=OldFaithful)\rplot(fit7)\rfit8 \u0026lt;- locfit(TimeWaiting~lp(TimeEruption,deg=0),data=OldFaithful)\rplot(fit8)\rhist(OldFaithful$TimeEruption,freq=FALSE)\rdev.copy(png,\u0026#39;oldfaithful_TimeWaitingvseruption02.png\u0026#39;,width = 800, height = 800)\rdev.off()\rGraphs and Highlights Blow graph shows the minimum points where we can find the best nn value to be used for plotting the right curve.\nPart of the code to generate the above 2 graphs is from line 43 to line 61\n## cross-validation of smoothing constant\r ## for waiting time to next eruption\r alpha\u0026lt;-seq(0.20,1,by=0.01)\rn1=length(alpha)\rg=matrix(nrow=n1,ncol=4)\rfor (k in 1:length(alpha)) {\rg[k,]\u0026lt;-gcv(~lp(TimeWaiting,nn=alpha[k]),data=OldFaithful)\r}\rg\r#gcv is used to estimate the penalty coefficient from the generalized cross-validation criteria. https://www.rdocumentation.org/packages/SpatialExtremes/versions/2.0-7/topics/gcv\r plot(g[,4]~g[,3],ylab=\u0026#34;GCV\u0026#34;,xlab=\u0026#34;degrees of freedom\u0026#34;)\r#the minimum point of the curve indicate the best value of nn. In this case, we can find the minimum value point.\r which.min(g[,4])\r#This indicate\r nn=alpha[which.min(g[,4])] #this is the value of the minimum nn.\r fit5 \u0026lt;- locfit(~lp(TimeWaiting,nn=nn,deg=2),data=OldFaithful)\rplot(fit5)\r","date":1572015519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572015519,"objectID":"22a12a8960525d31f7feca71bea2dc8c","permalink":"/post/2019-10-25-ra-polynomial/","publishdate":"2019-10-25T22:58:39+08:00","relpermalink":"/post/2019-10-25-ra-polynomial/","section":"post","summary":"What is Polynomial Regression? and when to use it There is a good set of internet articles that well describe the Math behind the Polynomial regression. Some of good references can be with the following links\nhttps://en.wikipedia.org/wiki/Local_regression\nhttps://onlinelibrary.wiley.com/doi/10.1002/9781118596289.ch4\nWe use polynomial regression in cases when observing histogram, scatter plot, or distribution of data that are not well distributed or clustered into two or more than two groups as shown in below figure.","tags":["Regression Analysis","Polynomial"],"title":"Polynomial Regression – Volcano Eruption","type":"post"},{"authors":null,"categories":null,"content":"This example shows another very simple regression analysis using data of secondhand Toyota car prices. Code is copied from the book “Data Mining and Business Analytics with R” with minor modification on the graphs.\nThe regressions are done treating Price of Cars as functions of predictors such as Car weight, Model types, Number of cylinders, etc.\nFocus is on: (1) Select a trained set of data that gives Mean Error close to 0 as possible; (2) Cross Validate the data to provide an insight on better Predictors to be used in the model.\nData. #toyota \u0026lt;- read.csv(\u0026#34;ToyotaCorolla.csv\u0026#34;)\r toyota[1:3,]\rPrice Age KM FuelType HP MetColor Automatic CC Doors Weight\r1 13500 23 46986 Diesel 90 1 0 2000 3 1165\r2 13750 23 72937 Diesel 90 1 0 2000 3 1165\r3 13950 24 41711 Diesel 90 1 0 2000 3 1165\rCODE ## first we read in the data\r toyota \u0026lt;- read.csv(\u0026#34;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/ToyotaCorolla.csv\u0026#34;)\r#toyota \u0026lt;- read.csv(\u0026#34;ToyotaCorolla.csv\u0026#34;)\r toyota[1:3,]\rsummary(toyota)\rhist(toyota$Price)\r## next we create indicator variables for the categorical variable\r ## FuelType with its three nominal outcomes: CNG, Diesel, and Petrol\r v1=rep(1,length(toyota$FuelType))\rv2=rep(0,length(toyota$FuelType))\rtoyota$FuelType1=ifelse(toyota$FuelType==\u0026#34;CNG\u0026#34;,v1,v2) #return value of CNG to v1, otherwise 0\r toyota$FuelType2=ifelse(toyota$FuelType==\u0026#34;Diesel\u0026#34;,v1,v2) #return value of Diesel to v1, otherwise 0\r auto=toyota[-4] #ignore column 4 (Fueltype)\r auto[1:3,]\rplot.new()\r#par(mfrow=c(1,1))\r par(mar=c(4,4,1,1)+0.1,mfrow=c(4,2),bg=\u0026#34;white\u0026#34;,cex = 0.7, cex.main = 1)\rplot(Price~Age,data=auto)\rplot(Price~KM,data=auto)\rplot(Price~HP,data=auto)\rplot(Price~MetColor,data=auto)\rplot(Price~Automatic,data=auto)\rplot(Price~CC,data=auto)\rplot(Price~Doors,data=auto)\rplot(Price~Weight,data=auto)\rdev.copy(png,\u0026#39;toyota_xyplot.png\u0026#39;,width = 500, height = 800)\rdev.off()\r## regression on all data\r m1=lm(Price~.,data=auto)\rsummary(m1)\rset.seed(1)\r## fixing the seed value for the random selection guarantees the\r ## same results in repeated runs\r n=length(auto$Price)\rn1=1000\rn2=n-n1\rtrain=sample(1:n,n1) # generate random n1 integer number of data among the size from 1 to n.\r ## regression on training set\r m1=lm(Price~.,data=auto[train,])\rsummary(m1)\rpred=predict(m1,newdat=auto[-train,]) #adding a set of n2 number into regression model\r obs=auto$Price[-train]\rdiff=obs-pred\rpercdiff=abs(diff)/obs\rme=mean(diff)\rrmse=sqrt(sum(diff**2)/n2)\rmape=100*(mean(percdiff))\rme # mean error\r rmse # root mean square error\r mape # mean absolute percent error\r ## cross-validation (leave one out)\r n=length(auto$Price)\rdiff=dim(n)\rpercdiff=dim(n)\rfor (k in 1:n) {\rtrain1=c(1:n)\rtrain=train1[train1!=k]\rm1=lm(Price~.,data=auto[train,])\rpred=predict(m1,newdat=auto[-train,])\robs=auto$Price[-train]\rdiff[k]=obs-pred\rpercdiff[k]=abs(diff[k])/obs\r}\rme=mean(diff)\rrmse=sqrt(mean(diff**2))\rmape=100*(mean(percdiff))\rme # mean error\r rmse # root mean square error\r mape # mean absolute percent error\r ## cross-validation (leave one out): Model with just Age\r n=length(auto$Price)\rdiff=dim(n)\rpercdiff=dim(n)\rfor (k in 1:n) {\rtrain1=c(1:n)\rtrain=train1[train1!=k]\rm1=lm(Price~Age,data=auto[train,])\rpred=predict(m1,newdat=auto[-train,])\robs=auto$Price[-train]\rdiff[k]=obs-pred\rpercdiff[k]=abs(diff[k])/obs\r}\rme=mean(diff)\rrmse=sqrt(mean(diff**2))\rmape=100*(mean(percdiff))\rme # mean error\r rmse # root mean square error\r mape # mean absolute percent error\r ## Adding the squares of Age and KM to the model\r auto$Age2=auto$Age^2\rauto$KM2=auto$KM^2\rm11=lm(Price~Age+KM,data=auto)\rsummary(m11)\rm12=lm(Price~Age+Age2+KM+KM2,data=auto)\rsummary(m12)\rm13=lm(Price~Age+Age2+KM,data=auto)\rsummary(m13)\r#----------------\r plot.new()\r#par(mfrow=c(1,1))\r par(mar=c(4,4,1,1)+0.1,mfrow=c(2,2),bg=\u0026#34;white\u0026#34;,cex = 0.7, cex.main = 1)\rplot(m11$res~m11$fitted)\rhist(m11$res)\rplot(m12$res~m12$fitted)\rdev.copy(png,\u0026#39;toyota_m11m12.png\u0026#39;,width = 800, height = 500)\rdev.off()\rGraphs and Outcomes Highlights Function ifelse() is used to return the logical value YES or NO.\n## next we create indicator variables for the categorical variable\r ## FuelType with its three nominal outcomes: CNG, Diesel, and Petrol\r v1=rep(1,length(toyota$FuelType))\rv2=rep(0,length(toyota$FuelType))\rtoyota$FuelType1=ifelse(toyota$FuelType==\u0026#34;CNG\u0026#34;,v1,v2) #return value of CNG to v1, otherwise 0\r toyota$FuelType2=ifelse(toyota$FuelType==\u0026#34;Diesel\u0026#34;,v1,v2) #return value of Diesel to v1, otherwise 0\r m1=lm(Price~.,data=auto)\rsummary(m1)\rCall:\rlm(formula = Price ~ ., data = auto)\rResiduals:\rMin 1Q Median 3Q Max\r-10642.3 -737.7 3.1 731.3 6451.5\rCoefficients:\rEstimate Std. Error t value Pr(\u0026amp;gt;|t|)\r(Intercept) -2.681e+03 1.219e+03 -2.199 0.028036 *\rAge -1.220e+02 2.602e+00 -46.889 \u0026lt; 2e-16 ***\rKM -1.621e-02 1.313e-03 -12.347 \u0026lt; 2e-16 ***\rHP 6.081e+01 5.756e+00 10.565 \u0026lt; 2e-16 ***\rMetColor 5.716e+01 7.494e+01 0.763 0.445738\rAutomatic 3.303e+02 1.571e+02 2.102 0.035708 *\rCC -4.174e+00 5.453e-01 -7.656 3.53e-14 ***\rDoors -7.776e+00 4.006e+01 -0.194 0.846129\rWeight 2.001e+01 1.203e+00 16.629 \u0026lt; 2e-16 ***\rFuelType1 -1.121e+03 3.324e+02 -3.372 0.000767 ***\rFuelType2 2.269e+03 4.394e+02 5.164 2.75e-07 ***\r---\rSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\rResidual standard error: 1316 on 1425 degrees of freedom\rMultiple R-squared: 0.8693, Adjusted R-squared: 0.8684\rF-statistic: 948 on 10 and 1425 DF, p-value: \u0026lt; 2.2e-16\rset.seed(1)\r## fixing the seed value for the random selection guarantees the\r ## same results in repeated runs\r n=length(auto$Price)\rn1=1000\rn2=n-n1\rtrain=sample(1:n,n1) # generate random n1 integer number of data among the size from 1 to n.\r ## regression on training set\r m1=lm(Price~.,data=auto[train,])\rsummary(m1)\rCall:\rlm(formula = Price ~ ., data = auto[train, ])\rResiduals:\rMin 1Q Median 3Q Max\r-8914.6 -778.2 -22.0 751.4 6480.4\rCoefficients:\rEstimate Std. Error t value Pr(\u0026amp;gt;|t|)\r(Intercept) 5.337e+02 1.417e+03 0.377 0.706\rAge -1.233e+02 3.184e+00 -38.725 \u0026lt; 2e-16 ***\rKM -1.726e-02 1.585e-03 -10.892 \u0026lt; 2e-16 ***\rHP 5.472e+01 7.662e+00 7.142 1.78e-12 ***\rMetColor 1.581e+02 9.199e+01 1.719 0.086 .\rAutomatic 2.703e+02 1.982e+02 1.364 0.173\rCC -3.634e+00 7.031e-01 -5.168 2.86e-07 ***\rDoors 3.828e+01 4.851e+01 0.789 0.430\rWeight 1.671e+01 1.379e+00 12.118 \u0026lt; 2e-16 ***\rFuelType1 -5.950e+02 4.366e+02 -1.363 0.173\rFuelType2 2.279e+03 5.582e+02 4.083 4.80e-05 ***\r---\rSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\rResidual standard error: 1343 on 989 degrees of freedom\rMultiple R-squared: 0.8573, Adjusted R-squared: 0.8559\rF-statistic: 594.3 on 10 and 989 DF, p-value: \u0026lt; 2.2e-16\r## regression on training set\r m1=lm(Price~.,data=auto[train,])\rsummary(m1)\rpred=predict(m1,newdat=auto[-train,]) #adding a set of n2 number into regression model\r obs=auto$Price[-train]\rdiff=obs-pred\rpercdiff=abs(diff)/obs\rme=mean(diff)\rrmse=sqrt(sum(diff**2)/n2)\rmape=100*(mean(percdiff))\rme # mean error\r rmse # root mean square error\r mape # mean absolute percent error\r \u0026gt; me # mean error\r [1] -48.70784\r\u0026gt; rmse # root mean square error\r [1] 1283.097\r\u0026gt; mape # mean absolute percent error\r [1] 9.208957\rThe above results give quite high value of Mean Errors, indicating a certain level of bias\n## cross-validation (leave one out)\r n=length(auto$Price)\rdiff=dim(n)\rpercdiff=dim(n)\rfor (k in 1:n) {\rtrain1=c(1:n)\rtrain=train1[train1!=k]\rm1=lm(Price~.,data=auto[train,])\rpred=predict(m1,newdat=auto[-train,])\robs=auto$Price[-train]\rdiff[k]=obs-pred\rpercdiff[k]=abs(diff[k])/obs\r}\rme=mean(diff)\rrmse=sqrt(mean(diff**2))\rmape=100*(mean(percdiff))\rme # mean error\r rmse # root mean square error\r mape # mean absolute percent error\r \u0026gt; me # mean error\r [1] -2.726251\r\u0026gt; rmse # root mean square error\r [1] 1354.509\r\u0026gt; mape # mean absolute percent error\r [1] 9.530529\rNow, the Mean Error is -2.7, which is a lot better than that of previous model. We can still further improve\n## cross-validation (leave one out): Model with just Age\r n=length(auto$Price)\rdiff=dim(n)\rpercdiff=dim(n)\rfor (k in 1:n) {\rtrain1=c(1:n)\rtrain=train1[train1!=k]\rm1=lm(Price~Age,data=auto[train,])\rpred=predict(m1,newdat=auto[-train,])\robs=auto$Price[-train]\rdiff[k]=obs-pred\rpercdiff[k]=abs(diff[k])/obs\r}\rme=mean(diff)\rrmse=sqrt(mean(diff**2))\rmape=100*(mean(percdiff))\rme # mean error\r rmse # root mean square error\r mape # mean absolute percent error\r \u0026gt; me # mean error\r [1] 0.6085014\r\u0026gt; rmse # root mean square error\r [1] 1748.76\r\u0026gt; mape # mean absolute percent error\r [1] 12.13156\rSee, the mean value of 0.6 now. Surely better than previous one.\n","date":1571931045,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571931045,"objectID":"de6673903df0fd19f16b79b2ea172a2e","permalink":"/post/2019-10-24-ra-price-2nd-toyota/","publishdate":"2019-10-24T23:30:45+08:00","relpermalink":"/post/2019-10-24-ra-price-2nd-toyota/","section":"post","summary":"This example shows another very simple regression analysis using data of secondhand Toyota car prices. Code is copied from the book “Data Mining and Business Analytics with R” with minor modification on the graphs.\nThe regressions are done treating Price of Cars as functions of predictors such as Car weight, Model types, Number of cylinders, etc.\nFocus is on: (1) Select a trained set of data that gives Mean Error close to 0 as possible; (2) Cross Validate the data to provide an insight on better Predictors to be used in the model.","tags":["Regression Analysis"],"title":"Price of 2nd hand Toyota cars - Regression Analysis","type":"post"},{"authors":[],"categories":["Project Management","Plant Audit","Asset Management"],"content":" Client: Maynilad Water Services Inc. Name: Rehabilitation, Retrofitting and Process Improvement of Lamesa 2 Water Treatment Plant Theme: Asset Management, Process Improvement, Retrofitting, Rehabilitation Scale: 900 MLD Timeline: Engineering (Jun/2016-Dec/2016), Procurement and Bidding (Aug/2016-Sep/2017), Construction (Oct/2017 onward)  Objectives  Upgrade the existing Pulsator WTP (900MLD) to cope with  High turbidity events in raining seasons (e.g. NTU up to 2000); Slow performance of Sludge Extraction System under high turbidity events; Rapidlly deterioration and corrosion of structures and assets; 7.2 magnitute of earthquake.   Provide better integration and connectivity with the Lamesa 1 Water Treatment Plant (1200 MLD) and the Sludge Treatment Facility (STF) being designed and constructed by JFE Contractor. Provide fully Automation and integration (e.g. SCADA)  Roles and Responsibilities  Project Management  Lesson Learned Authorship Back to Projects\n","date":1571454502,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571454502,"objectID":"d3985886bb985f35c849fd60437363a1","permalink":"/project/2016-lamesa2/","publishdate":"2019-10-19T11:08:22+08:00","relpermalink":"/project/2016-lamesa2/","section":"project","summary":"Rehabilitation, Retrofitting, and Process Improvement of Lamesa 2 Water Treatment Plant","tags":["Project Management","Plant Audit","Asset Management","Water"],"title":"Lamesa 2 - Maynilad","type":"project"},{"authors":[],"categories":["Project Management","Plant Audit","Asset Management"],"content":"The Project  Client: Maynilad Water Services Inc. Name: Plant Audit of Various Pump Stations and Reservoirs Theme: Asset Management  Objectives  Visual Inspection and Testing on mechanical assets (e.g. pump, piping, valves) and electrical assets (e.g. GENSETs, transformers, MCC, Motors) using developed set condition states and standards; Energy Audit; FDAS and WEM studies; Determination of optimal 5 years intervention program; Life cycle cost analysis and estimation of Return on Investment.  Responsibilities  Record inspection and testing data in a centralized relational database system (MySQL); Big data analysis and data solutions on 5-10 years operation data to detect trend and reliability of the system; Investigation of current design and operational scheme using qualitative reliability analysis (e.g. FMEA); Weibull Analysis and Faul Tree Analysis to estimate reliability; Development of a Life Cycle Cost model using reliability concept to determine optimal intervention strategies and program; Organizing a series of training and technical workshops on reliability study and modelling used in the project.  Lesson Learned Authorship  Nam led the team and trained the team to use LaTEX system to compile all reports  Back to Projects\n","date":1571454502,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571454502,"objectID":"44d0d05e3debf4fb462bb33a5a870324","permalink":"/project/2018-plantaudit/","publishdate":"2019-10-19T11:08:22+08:00","relpermalink":"/project/2018-plantaudit/","section":"project","summary":"Plant Audit and Asset Management for Various Pump Stations and Reservoirs","tags":["Project Management","Plant Audit","Asset Management","Water"],"title":"Plant Audit - Maynilad","type":"project"},{"authors":null,"categories":null,"content":"This example shows a very simple regression analysis using fuel data of 38 cars. Code is copied from the book “Data Mining and Business Analytics with R” with minor modification on the graphs.\nThere are 2 learning points to be remembered with this example.\nLeaps package with regsubsets fuction. Regsubsets function allows to perform regression on subsets of data, particularly useful to compare across model’s predictors to understand the fit of the model.\nhttps://www.rdocumentation.org/packages/leaps/versions/2.1-1/topics/regsubsets\nCross-Validation  Cross-validation removes one case from the data set of n cases, fits the model to the reduced data set, and predicts the response of that one case that has been removed from the estimation. This is repeated for each of the n cases. The summary statistics of the n genuine out-of-sample prediction errors (mean error, root mean square error, mean absolute percent error) help us assess the out-of-sample prediction performance. Cross-validation is very informative as it evaluates the model on new data. We find that the model with all six regressors performs better. It leads to a mean absolute percent error of about 6.75% (as compared to 8.23% for the model with weight as the only regressor).\n \u0026gt; FuelEff\rGPM WT DIS NC HP ACC ET\r1 5.917 4.360 350 8 155 14.9 1\r2 6.452 4.054 351 8 142 14.3 1\r3 5.208 3.605 267 8 125 15.0 1\r4 5.405 3.940 360 8 150 13.0 1\r5 3.333 2.155 98 4 68 16.5 0\r6 3.636 2.560 134 4 95 14.2 0\r7 3.676 2.300 119 4 97 14.7 0\r8 3.236 2.230 105 4 75 14.5 0\r9 4.926 2.830 131 5 103 15.9 0\r10 5.882 3.140 163 6 125 13.6 0\r11 4.630 2.795 121 4 115 15.7 0\r12 6.173 3.410 163 6 133 15.8 0\r13 4.854 3.380 231 6 105 15.8 0\r14 4.808 3.070 200 6 85 16.7 0\r15 5.376 3.620 225 6 110 18.7 0\r16 5.525 3.410 258 6 120 15.1 0\r17 5.882 3.840 305 8 130 15.4 1\r18 5.682 3.725 302 8 129 13.4 1\r19 6.061 3.955 351 8 138 13.2 1\r20 5.495 3.830 318 8 135 15.2 1\r21 3.774 2.585 140 4 88 14.4 0\r22 4.566 2.910 171 6 109 16.6 1\r23 2.933 1.975 86 4 65 15.2 0\r24 2.849 1.915 98 4 80 14.4 0\r25 3.650 2.670 121 4 80 15.0 0\r26 3.175 1.990 89 4 71 14.9 0\r27 3.390 2.135 98 4 68 16.6 0\r28 3.521 2.670 151 4 90 16.0 0\r29 3.472 2.595 173 6 115 11.3 1\r30 3.731 2.700 173 6 115 12.9 1\r31 2.985 2.556 151 4 90 13.2 0\r32 2.924 2.200 105 4 70 13.2 0\r33 3.145 2.020 85 4 65 19.2 0\r34 2.681 2.130 91 4 69 14.7 0\r35 3.279 2.190 97 4 78 14.1 0\r36 4.545 2.815 146 6 97 14.5 0\r37 4.651 2.600 121 4 110 12.8 0\r38 3.135 1.925 89 4 71 14.0 0\rCODE ## first we read in the data\r FuelEff \u0026lt;- read.csv(\u0026#34;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/FuelEfficiency.csv\u0026#34;)\r#FuelEff \u0026lt;- read.csv(\u0026#34;FuelEfficiency.csv\u0026#34;)\r FuelEff\r#----------------------------\r plot.new()\r#par(mfrow=c(1,1))\r par(mar=c(4,4,1,1)+0.1,mfrow=c(3,3),bg=\u0026#34;white\u0026#34;,cex = 1, cex.main = 1)\rplot(GPM~MPG,data=FuelEff)\rplot(GPM~WT,data=FuelEff)\rplot(GPM~DIS,data=FuelEff)\rplot(GPM~NC,data=FuelEff)\rplot(GPM~HP,data=FuelEff)\rplot(GPM~ACC,data=FuelEff)\rplot(GPM~ET,data=FuelEff)\rdev.copy(png,\u0026#39;fueleff_xyplot.png\u0026#39;,width = 800, height = 800)\rdev.off()\rFuelEff=FuelEff[-1] #ignore the MPG column\r FuelEff\r## regression on all data\r m1=lm(GPM~.,data=FuelEff)\rsummary(m1)\rcor(FuelEff)\r## best subset regression in R\r library(leaps)\rX=FuelEff[,2:7]\ry=FuelEff[,1]\r#use the regsubsets function from package leaps to compute regression of the subsets\r #https://www.rdocumentation.org/packages/leaps/versions/2.1-1/topics/regsubsets\r out=summary(regsubsets(X,y,nbest=2,nvmax=ncol(X)))\rtab=cbind(out$which,out$rsq,out$adjr2,out$cp)\rtab\rm2=lm(GPM~WT,data=FuelEff)\rsummary(m2)\r## cross-validation (leave one out) for the model on all six regressors\r n=length(FuelEff$GPM)\rdiff=dim(n)\rpercdiff=dim(n)\rfor (k in 1:n) {\rtrain1=c(1:n)\rtrain=train1[train1!=k]\r## the R expression \u0026#34;train1[train1!=k]\u0026#34; picks from train1 those\r ## elements that are different from k and stores those elements in the\r ## object train.\r ## For k=1, train consists of elements that are different from 1; that\r ## is 2, 3, …, n.\r m1=lm(GPM~.,data=FuelEff[train,])\rpred=predict(m1,newdat=FuelEff[-train,]) #adding the new data, which is ignored earlier\r obs=FuelEff$GPM[-train]\rdiff[k]=obs-pred\rpercdiff[k]=abs(diff[k])/obs\r}\rme=mean(diff)\rrmse=sqrt(mean(diff**2))\rmape=100*(mean(percdiff))\rme # mean error\r rmse # root mean square error\r mape # mean absolute percent error\r ## cross-validation (leave one out) for the model on weight only\r n=length(FuelEff$GPM)\rdiff=dim(n)\rpercdiff=dim(n)\rfor (k in 1:n) {\rtrain1=c(1:n)\rtrain=train1[train1!=k]\rm2=lm(GPM~WT,data=FuelEff[train,])\rpred=predict(m2,newdat=FuelEff[-train,])\robs=FuelEff$GPM[-train]\rdiff[k]=obs-pred\rpercdiff[k]=abs(diff[k])/obs\r}\rme=mean(diff)\rrmse=sqrt(mean(diff**2))\rmape=100*(mean(percdiff))\rme # mean error\r rmse # root mean square error\r mape # mean absolute percent error\r Graphs and Outcomes ## regression on all data\r m1=lm(GPM~.,data=FuelEff)\rsummary(m1)\rCall:\rlm(formula = GPM ~ ., data = FuelEff)\rResiduals:\rMin 1Q Median 3Q Max\r-0.4996 -0.2547 0.0402 0.1956 0.6455\rCoefficients:\rEstimate Std. Error t value Pr(\u0026amp;gt;|t|)\r(Intercept) -2.599357 0.663403 -3.918 0.000458 ***\rWT 0.787768 0.451925 1.743 0.091222 .\rDIS -0.004890 0.002696 -1.814 0.079408 .\rNC 0.444157 0.122683 3.620 0.001036 **\rHP 0.023599 0.006742 3.500 0.001431 **\rACC 0.068814 0.044213 1.556 0.129757\rET -0.959634 0.266785 -3.597 0.001104 **\r---\rSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\rResidual standard error: 0.313 on 31 degrees of freedom\rMultiple R-squared: 0.9386, Adjusted R-squared: 0.9267\rF-statistic: 78.94 on 6 and 31 DF, p-value: \u0026lt; 2.2e-16\rcor(FuelEff)\rGPM WT DIS NC HP ACC ET\rGPM 1.00000000 0.92626656 0.8229098 0.8411880 0.8876992 0.03307093 0.5206121\rWT 0.92626656 1.00000000 0.9507647 0.9166777 0.9172204 -0.03357386 0.6673661\rDIS 0.82290984 0.95076469 1.0000000 0.9402812 0.8717993 -0.14341745 0.7746636\rNC 0.84118805 0.91667774 0.9402812 1.0000000 0.8638473 -0.12924363 0.8311721\rHP 0.88769915 0.91722045 0.8717993 0.8638473 1.0000000 -0.25262113 0.7202350\rACC 0.03307093 -0.03357386 -0.1434174 -0.1292436 -0.2526211 1.00000000 -0.3102336\rET 0.52061208 0.66736606 0.7746636 0.8311721 0.7202350 -0.31023357 1.0000000\r## best subset regression in R\r library(leaps)\rX=FuelEff[,2:7]\ry=FuelEff[,1]\rout=summary(regsubsets(X,y,nbest=2,nvmax=ncol(X)))\rtab=cbind(out$which,out$rsq,out$adjr2,out$cp)\rtab\r(Intercept) WT DIS NC HP ACC ET\r1 1 1 0 0 0 0 0 0.8579697 0.8540244 37.674750\r1 1 0 0 0 1 0 0 0.7880098 0.7821212 72.979632\r2 1 1 1 0 0 0 0 0.8926952 0.8865635 22.150747\r2 1 1 0 0 0 0 1 0.8751262 0.8679906 31.016828\r3 1 0 0 1 1 0 1 0.9145736 0.9070360 13.109930\r3 1 1 1 1 0 0 0 0.9028083 0.8942326 19.047230\r4 1 0 0 1 1 1 1 0.9313442 0.9230223 6.646728\r4 1 1 0 1 1 0 1 0.9204005 0.9107520 12.169443\r5 1 1 1 1 1 0 1 0.9337702 0.9234218 7.422476\r5 1 0 1 1 1 1 1 0.9325494 0.9220103 8.038535\r6 1 1 1 1 1 1 1 0.9385706 0.9266810 7.000000\rm2=lm(GPM~WT,data=FuelEff)\rsummary(m2)\rCall:\rlm(formula = GPM ~ WT, data = FuelEff)\rResiduals:\rMin 1Q Median 3Q Max\r-0.88072 -0.29041 0.00659 0.19021 1.13164\rCoefficients:\rEstimate Std. Error t value Pr(\u0026amp;gt;|t|)\r(Intercept) -0.006101 0.302681 -0.02 0.984\rWT 1.514798 0.102721 14.75 \u0026lt;2e-16 ***\r---\rSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\rResidual standard error: 0.4417 on 36 degrees of freedom\rMultiple R-squared: 0.858, Adjusted R-squared: 0.854\rF-statistic: 217.5 on 1 and 36 DF, p-value: \u0026lt; 2.2e-16\r","date":1570721445,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570721445,"objectID":"54e428881124b61dde9eb6ea6f7421a7","permalink":"/post/2019-10-10-fuel-efficiency/","publishdate":"2019-10-10T23:30:45+08:00","relpermalink":"/post/2019-10-10-fuel-efficiency/","section":"post","summary":"This example shows a very simple regression analysis using fuel data of 38 cars. Code is copied from the book “Data Mining and Business Analytics with R” with minor modification on the graphs.\nThere are 2 learning points to be remembered with this example.\nLeaps package with regsubsets fuction. Regsubsets function allows to perform regression on subsets of data, particularly useful to compare across model’s predictors to understand the fit of the model.","tags":["Regression Analysis"],"title":"Fuel Efficiency - Regression Analysis","type":"post"},{"authors":null,"categories":null,"content":"In many cases, we need to remove space in the names of files saved inside a tree of folders/subfolders. We can use following CODE to do the job.\n:renameNoSpace [/R]\r@echo off\rsetlocal disableDelayedExpansion\rif /i \u0026quot;%~1\u0026quot;==\u0026quot;/R\u0026quot; (\rset \u0026quot;forOption=%~1 %2\u0026quot;\rset \u0026quot;inPath=\u0026quot;\r) else (\rset \u0026quot;forOption=\u0026quot;\rif \u0026quot;%~1\u0026quot; neq \u0026quot;\u0026quot; (set \u0026quot;inPath=%~1\\\u0026quot;) else set \u0026quot;inPath=\u0026quot;\r)\rfor %forOption% %%F in (\u0026quot;%inPath%* *\u0026quot;) do (\rif /i \u0026quot;%~f0\u0026quot; neq \u0026quot;%%~fF\u0026quot; (\rset \u0026quot;folder=%%~dpF\u0026quot;\rset \u0026quot;file=%%~nxF\u0026quot;\rsetlocal enableDelayedExpansion\recho ren \u0026quot;!folder!!file!\u0026quot; \u0026quot;!file: =!\u0026quot;\rren \u0026quot;!folder!!file!\u0026quot; \u0026quot;!file: =!\u0026quot;\rendlocal\r)\r)\rAssume the script is called renameNoSpace.bat\nOptions to change are\nrenameNoSpace : (no arguments) Renames files in the current directory\rrenameNoSpace /R : Renames files in the folder tree rooted at the current directory\rrenameNoSpace myFolder : Renames files in the “myFolder” directory found in the current directory.\rrenameNoSpace \u0026quot;c:\\my folder\\\u0026quot; : Renames files in the specified path. Quotes are used because path contains a space.\rrenameNoSpace /R c:\\ : Renames all files on the C: drive.\rWe paste renameNoSpace.bat to targetted folder/subfolders and run the file in the console.\n","date":1570721445,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570721445,"objectID":"dd2505d1189f072382cd00da3784c5bf","permalink":"/post/2019-10-10-remove-space-in-bulk/","publishdate":"2019-10-10T23:30:45+08:00","relpermalink":"/post/2019-10-10-remove-space-in-bulk/","section":"post","summary":"In many cases, we need to remove space in the names of files saved inside a tree of folders/subfolders. We can use following CODE to do the job.\n:renameNoSpace [/R]\r@echo off\rsetlocal disableDelayedExpansion\rif /i \u0026quot;%~1\u0026quot;==\u0026quot;/R\u0026quot; (\rset \u0026quot;forOption=%~1 %2\u0026quot;\rset \u0026quot;inPath=\u0026quot;\r) else (\rset \u0026quot;forOption=\u0026quot;\rif \u0026quot;%~1\u0026quot; neq \u0026quot;\u0026quot; (set \u0026quot;inPath=%~1\\\u0026quot;) else set \u0026quot;inPath=\u0026quot;\r)\rfor %forOption% %%F in (\u0026quot;%inPath%* *\u0026quot;) do (\rif /i \u0026quot;%~f0\u0026quot; neq \u0026quot;%%~fF\u0026quot; (\rset \u0026quot;folder=%%~dpF\u0026quot;\rset \u0026quot;file=%%~nxF\u0026quot;\rsetlocal enableDelayedExpansion\recho ren \u0026quot;!","tags":["Window"],"title":"Removing Space of files inside folders/subfolders in bulk","type":"post"},{"authors":null,"categories":null,"content":"This is an example presented in the book “Data Mining and Business Analytics with R”, with some useful basic graphs that can be reused for other sets of similar data. Code and Data are saved in Github link\nstore brand week logmove feat price AGE60 EDUC ETHNIC INCOME HHLARGE\r1 2 tropicana 40 9.018695 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r2 2 tropicana 46 8.723231 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r3 2 tropicana 47 8.253228 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r4 2 tropicana 48 8.987197 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r5 2 tropicana 50 9.093357 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r6 2 tropicana 51 8.877382 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r7 2 tropicana 52 9.294682 0 3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r8 2 tropicana 53 8.954674 0 3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r9 2 tropicana 54 9.049232 0 3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r10 2 tropicana 57 8.613230 0 3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r11 2 tropicana 58 8.680672 0 3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r12 2 tropicana 59 9.034080 0 3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r13 2 tropicana 60 8.691483 0 3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r14 2 tropicana 61 8.831712 0 3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r15 2 tropicana 62 9.128696 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r16 2 tropicana 63 9.405907 0 2.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r17 2 tropicana 64 9.447150 0 2.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r18 2 tropicana 65 8.783856 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r19 2 tropicana 66 8.723231 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r20 2 tropicana 67 9.957976 0 2.39 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r21 2 tropicana 68 9.426741 0 2.39 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r22 2 tropicana 69 9.156095 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r23 2 tropicana 70 9.793673 0 2.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r24 2 tropicana 71 9.149316 0 2.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r25 2 tropicana 72 8.743851 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r26 2 tropicana 73 8.841014 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r27 2 tropicana 74 9.727228 0 2.49 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r28 2 tropicana 75 8.743851 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r29 2 tropicana 76 8.979165 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r30 2 tropicana 77 8.723231 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r31 2 tropicana 78 8.979165 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r32 2 tropicana 79 8.962904 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r33 2 tropicana 80 8.712760 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r34 2 tropicana 81 10.649607 1 1.69 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r35 2 tropicana 82 8.502689 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r36 2 tropicana 83 10.292281 1 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r37 2 tropicana 84 9.208739 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r38 2 tropicana 85 10.468801 1 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r39 2 tropicana 86 10.083139 0 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r40 2 tropicana 87 8.868413 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r41 2 tropicana 88 10.106918 1 2.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r42 2 tropicana 89 8.754003 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r43 2 tropicana 90 8.712760 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r44 2 tropicana 91 10.420375 0 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r45 2 tropicana 92 9.491602 0 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r46 2 tropicana 93 8.733594 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r47 2 tropicana 94 9.270871 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r48 2 tropicana 95 10.707102 0 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r49 2 tropicana 97 9.908276 0 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r50 2 tropicana 98 9.121728 1 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r51 2 tropicana 99 9.996614 0 2.19 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r52 2 tropicana 100 9.515469 0 2.19 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r53 2 tropicana 103 8.333270 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r54 2 tropicana 104 10.582130 1 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r55 2 tropicana 105 8.636220 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r56 2 tropicana 106 9.107643 1 2.68 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r57 2 tropicana 107 8.702178 0 3.44 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r58 2 tropicana 108 8.954674 0 3.14 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\rWORKWOM HVAL150 SSTRDIST SSTRVOL CPDIST5 CPWVOL5\r1 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r2 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r3 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r4 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r5 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r6 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r7 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r8 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r9 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r10 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r11 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r12 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r13 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r14 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r15 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r16 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r17 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r18 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r19 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r20 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r21 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r22 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r23 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r24 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r25 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r26 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r27 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r28 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r29 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r30 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r31 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r32 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r33 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r34 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r35 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r36 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r37 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r38 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r39 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r40 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r41 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r42 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r43 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r44 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r45 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r46 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r47 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r48 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r49 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r50 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r51 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r52 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r53 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r54 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r55 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r56 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r57 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r58 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266\r[ reached \u0026#39;max\u0026#39; / getOption(\u0026#34;max.print\u0026#34;) -- omitted 28889 rows ]\r## Install packages from CRAN; use any USA mirror\r library(lattice)\r#oj \u0026lt;- read.csv(\u0026#34;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/oj.csv\u0026#34;)\r oj \u0026lt;- read.csv(\u0026#34;oj.csv\u0026#34;)\roj$store \u0026lt;- factor(oj$store) #change numberic value of store into categorical data\r oj[1:2,]\rt1=tapply(oj$logmove,oj$brand,FUN=mean,na.rm=TRUE) #calculate the mean of each brand using logmove value.\r t1\rt2=tapply(oj$logmove,INDEX=list(oj$brand,oj$week),FUN=mean,na.rm=TRUE) #calculate the mean of logmove value based on index lists per week.\r t2\r#plot each graph as time serieas data per week.\r plot.new()\rpar(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2),bg=\u0026#34;white\u0026#34;,cex = 1, cex.main = 0.6)\rplot(t2[1,],type= \u0026#34;l\u0026#34;,xlab=\u0026#34;week\u0026#34;,ylab=\u0026#34;dominicks\u0026#34;,ylim=c(7,12),cex.axis = 1,las = 1)\rplot(t2[2,],type= \u0026#34;l\u0026#34;,xlab=\u0026#34;week\u0026#34;,ylab=\u0026#34;minute.maid\u0026#34;,ylim=c(7,12),cex.axis = 1,las = 1)\rplot(t2[3,],type= \u0026#34;l\u0026#34;,xlab=\u0026#34;week\u0026#34;,ylab=\u0026#34;tropicana\u0026#34;,ylim=c(7,12),cex.axis = 1,las = 1)\rdev.copy(png,\u0026#39;oj_weekmean01.png\u0026#39;,width = 1600, height = 600)\rdev.off()\r#-------------------------------\r #now we combine the three above graphs into one single graphs for ease of comparison\r logmove=c(t2[1,],t2[2,],t2[3,])\rweek1=c(40:160)\rweek=c(week1,week1,week1)\rbrand1=rep(1,121)\rbrand2=rep(2,121)\rbrand3=rep(3,121)\rbrand=c(brand1,brand2,brand3)\rplot.new()\rxyplot(logmove~week|factor(brand),type= \u0026#34;l\u0026#34;,layout=c(1,3),col=\u0026#34;black\u0026#34;)\rdev.copy(png,\u0026#39;oj_weekmean02.png\u0026#39;,width = 1000, height = 600)\rdev.off()\r#-----------------------------\r plot.new()\rpar(mfrow=c(1,1))\r#par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2),bg=\u0026#34;white\u0026#34;,cex = 1, cex.main = 0.6)\r boxplot(logmove~brand,data=oj) # compare logmove of 3 branch using boxplot\r dev.copy(png,\u0026#39;oj_logmovebrandboxplot.png\u0026#39;,width = 800, height = 600)\rdev.off()\rhistogram(~logmove|brand,data=oj,layout=c(1,3)) # compare logmove of 3 branch using histogram\r dev.copy(png,\u0026#39;oj_logmovebrandhist.png\u0026#39;,width = 1000, height = 600)\rdev.off()\ra1=densityplot(~logmove|brand,data=oj,layout=c(1,3),plot.points=FALSE) # compare logmove of 3 branch using density plot\r a2=densityplot(~logmove,groups=brand,data=oj,plot.points=FALSE) ## compare logmove of 3 branch using density plot in a one frame\r #using xyplot to see the spartial distribution of data weekly\r library(gridExtra) #this package allows to plot multiple graphs in the same plot despite the difference in plotting engines (e.g. ggplot or barchart)\r grid.arrange(a1, a2, ncol = 2) #display the two plot a and p\r dev.copy(png,\u0026#39;oj_logmovedensity.png\u0026#39;,width = 1000, height = 500)\rdev.off()\r#-------------------------------------------------\r xyplot(logmove~week,data=oj,col=\u0026#34;black\u0026#34;)\rdev.copy(png,\u0026#39;oj_logmoveweekspartial.png\u0026#39;,width = 1000, height = 500)\rdev.off()\r#---------------------------------\r xyplot(logmove~week|brand,data=oj,layout=c(1,3),col=\u0026#34;black\u0026#34;)\rdev.copy(png,\u0026#39;oj_logmoveweekspartialbrand.png\u0026#39;,width = 1000, height = 500)\rdev.off()\r#---------------------------------\r xyplot(logmove~price,data=oj,col=\u0026#34;black\u0026#34;)\rdev.copy(png,\u0026#39;oj_logmoveprice.png\u0026#39;,width = 1000, height = 500)\rdev.off()\r#---------------------------------\r xyplot(logmove~price|brand,data=oj,layout=c(1,3),col=\u0026#34;black\u0026#34;)\rdev.copy(png,\u0026#39;oj_logmovepricebrand.png\u0026#39;,width = 1000, height = 500)\rdev.off()\r#---------------------------------\r smoothScatter(oj$price,oj$logmove)\rdev.copy(png,\u0026#39;oj_logmovepricesmooth.png\u0026#39;,width = 1000, height = 500)\rdev.off()\r#---------------------------------\r a1=densityplot(~logmove,groups=feat, data=oj, plot.points=FALSE)\ra2=xyplot(logmove~price,groups=feat, data=oj)\rgrid.arrange(a1, a2, ncol = 2) #display the two plot a and p\r dev.copy(png,\u0026#39;oj_logmovepricegroupfeat.png\u0026#39;,width = 1200, height = 500)\rdev.off()\r#------------------------------------------------\r oj1=oj[oj$store == 5,]\rxyplot(logmove~week|brand,data=oj1,type=\u0026#34;l\u0026#34;,layout=c(1,3),col=\u0026#34;black\u0026#34;)\rdev.copy(png,\u0026#39;oj_logmovebrand.png\u0026#39;,width = 1200, height = 500)\rdev.off()\rxyplot(logmove~price,data=oj1,col=\u0026#34;black\u0026#34;)\rdev.copy(png,\u0026#39;oj_logmovepricexyplot.png\u0026#39;,width = 800, height = 500)\rdev.off()\rxyplot(logmove~price|brand,data=oj1,layout=c(1,3),col=\u0026#34;black\u0026#34;)\rdev.copy(png,\u0026#39;oj_logmovepricebrandxyplot.png\u0026#39;,width = 1000, height = 500)\rdev.off()\rdensityplot(~logmove|brand,groups=feat,data=oj1,plot.points=FALSE)\rdev.copy(png,\u0026#39;oj_logmovebranddenst.png\u0026#39;,width = 1200, height = 500)\rdev.off()\rxyplot(logmove~price|brand,groups=feat,data=oj1)\rdev.copy(png,\u0026#39;oj_logmovepricebrandxyplot.png\u0026#39;,width = 1200, height = 500)\rdev.off()\r#----------------------------\r t21=tapply(oj$INCOME,oj$store,FUN=mean,na.rm=TRUE)\rt21\rt21[t21==max(t21)]\rt21[t21==min(t21)]\roj1=oj[oj$store == 62,]\roj2=oj[oj$store == 75,]\roj3=rbind(oj1,oj2)\r#----------------------------------------\r a1=xyplot(logmove~price|store,data=oj3)\ra2=xyplot(logmove~price|store,groups=feat,data=oj3)\rgrid.arrange(a1, a2, ncol = 1) #display the two plot a and p\r dev.copy(png,\u0026#39;oj_logmovexyplotprice.png\u0026#39;,width = 500, height = 1000)\rdev.off()\r## store in the wealthiest neighborhood\r plot.new()\rpar(mar=c(4,4,1,1)+0.1,mfrow=c(1,2),bg=\u0026#34;white\u0026#34;,cex = 1, cex.main = 1)\rmhigh=lm(logmove~price,data=oj1)\rsummary(mhigh)\rplot(logmove~price,data=oj1,xlim=c(0,4),ylim=c(0,13), main=\u0026#34;62 = wealthiest store\u0026#34;)\rabline(mhigh)\r## store in the poorest neighborhood\r mlow=lm(logmove~price,data=oj2)\rsummary(mlow)\rplot(logmove~price,data=oj2,xlim=c(0,4),ylim=c(0,13), main=\u0026#34;75 = poorest store\u0026#34;)\rabline(mlow)\rdev.copy(png,\u0026#39;oj_logmovepriceoj2.png\u0026#39;,width = 1000, height = 300)\rdev.off()\rGraphs Mean of logmove over 121 weeks\nboxplot\nThe last two graphs also present the regression lines with summary of coefficients using lm function\nmhigh=lm(logmove~price,data=oj1)\rsummary(mhigh)\rCall:\rlm(formula = logmove ~ price, data = oj1)\rResiduals:\rMin 1Q Median 3Q Max\r-4.9557 -0.4934 0.1815 0.6557 2.4454\rCoefficients:\rEstimate Std. Error t value Pr(\u0026amp;gt;|t|)\r(Intercept) 9.15394 0.21112 43.359 \u0026lt;2e-16 ***\rprice -0.01461 0.08381 -0.174 0.862\r---\rSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\rResidual standard error: 1.142 on 349 degrees of freedom\rMultiple R-squared: 8.712e-05, Adjusted R-squared: -0.002778\rF-statistic: 0.03041 on 1 and 349 DF, p-value: 0.8617\rmlow=lm(logmove~price,data=oj2)\rsummary(mlow)\rCall:\rlm(formula = logmove ~ price, data = oj2)\rResiduals:\rMin 1Q Median 3Q Max\r-3.5235 -0.5606 0.0392 0.5090 2.4523\rCoefficients:\rEstimate Std. Error t value Pr(\u0026amp;gt;|t|)\r(Intercept) 10.87695 0.15184 71.63 \u0026lt;2e-16 ***\rprice -0.67222 0.06071 -11.07 \u0026lt;2e-16 ***\r---\rSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\rResidual standard error: 0.8383 on 352 degrees of freedom\rMultiple R-squared: 0.2584, Adjusted R-squared: 0.2563\rF-statistic: 122.6 on 1 and 352 DF, p-value: \u0026lt; 2.2e-16\r","date":1570635045,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570635045,"objectID":"d5e33e34da32df5a6cfbd62efe09c62a","permalink":"/post/2019-10-09-orange-juice/","publishdate":"2019-10-09T23:30:45+08:00","relpermalink":"/post/2019-10-09-orange-juice/","section":"post","summary":"This is an example presented in the book “Data Mining and Business Analytics with R”, with some useful basic graphs that can be reused for other sets of similar data. Code and Data are saved in Github link\nstore brand week logmove feat price AGE60 EDUC ETHNIC INCOME HHLARGE\r1 2 tropicana 40 9.018695 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534\r2 2 tropicana 46 8.723231 0 3.87 0.2328647 0.2489349 0.","tags":["R","Plot"],"title":"Orange Juice","type":"post"},{"authors":null,"categories":null,"content":"This is a summary of example 2 in Chapter 2 of the book “Data Mining and Business Analytics with R”. The post keeps the original code with some polishing syntax for better plotting, particularly using ggplot2 package and gridExtra.\nCode and Data are saved in Github link\nlibrary(lattice)\rlibrary(ggplot2)\r#Using multiple plot function when using with ggplot\r #source(\u0026#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/genericfunctions/multiplot.R\u0026#34;)\r #----1. Data\r don \u0026lt;- read.csv(\u0026#34;contribution.csv\u0026#34;)\r#or read directly from the web\r #don \u0026lt;- read.csv(\u0026#34;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/contribution.csv\u0026#34;)\r #or\r don[1:5,] #display the first 5 data rows\r table(don$Class.Year) #display total numbers of data points for each batch of year\r a=barchart(table(don$Class.Year),horizontal=FALSE,xlab=\u0026#34;Class Year\u0026#34;,col=\u0026#34;black\u0026#34;)\rp=ggplot(data.frame(table(don$Class.Year)), aes(x=Var1, y=Freq))+labs(y=\u0026#34;Freq\u0026#34;, x=\u0026#34;Class Year\u0026#34;) + geom_bar(stat=\u0026#34;identity\u0026#34;,width=0.8,color=\u0026#34;blue\u0026#34;,fill=\u0026#34;steelblue\u0026#34;)+geom_text(aes(label=Freq), vjust=-0.3, size=3.5)\rplot.new()\rpar(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(1,2),bg=\u0026#34;white\u0026#34;)\rlibrary(gridExtra) #this package allows to plot multiple graphs in the same plot despite the difference in plotting engines (e.g. ggplot or barchart)\r grid.arrange(a, p, ncol = 2) #display the two plot a and p\r dev.copy(png,\u0026#39;alumni_classyear_bar.png\u0026#39;,width = 1200, height = 500)\rdev.off()\r\u0026gt; don[1:5,]\rGender Class.Year Marital.Status Major Next.Degree FY04Giving FY03Giving FY02Giving\r1 M 1957 M History LLB 2500 2500 1400\r2 M 1957 M Physics MS 5000 5000 5000\r3 F 1957 M Music NONE 5000 5000 5000\r4 M 1957 M History NONE 0 5100 200\r5 M 1957 M Biology MD 1000 1000 1000\rFY01Giving FY00Giving AttendenceEvent\r1 12060 12000 1\r2 5000 10000 1\r3 5000 10000 1\r4 200 0 1\r5 1005 1000 1\r\u0026gt; table(don$Class.Year)\r1957 1967 1977 1987 1997\r127 222 243 277 361\rBarchart from Lattice package\ndon$TGiving=don$FY00Giving+don$FY01Giving+don$FY02Giving+don$FY03Giving+don$FY04Giving\rmean(don$TGiving)\rsd(don$TGiving)\rquantile(don$TGiving,probs=seq(0,1,0.05))\rquantile(don$TGiving,probs=seq(0.95,1,0.01))\rmean(don$TGiving)\r[1] 980.0436\r\u0026gt; sd(don$TGiving)\r[1] 6670.773\r\u0026gt; quantile(don$TGiving,probs=seq(0,1,0.05))\r0% 5% 10% 15% 20% 25% 30% 35% 40% 45%\r0.0 0.0 0.0 0.0 0.0 0.0 0.0 10.0 25.0 50.0\r50% 55% 60% 65% 70% 75% 80% 85% 90% 95%\r75.0 100.0 150.8 200.0 275.0 400.0 554.2 781.0 1050.0 2277.5\r100%\r171870.1\r\u0026gt; quantile(don$TGiving,probs=seq(0.95,1,0.01))\r95% 96% 97% 98% 99% 100%\r2277.50 3133.56 5000.00 7000.00 16442.14 171870.06\r#---------------------\r plot.new()\rpar(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2))\rhist(don$TGiving,main=NULL,xlab=\u0026#34;Total Contribution\u0026#34;) #histograph with outliners\r hist(don$TGiving[don$TGiving!=0][don$TGiving[don$TGiving!=0]\u0026lt;=1000],main=NULL,xlab=\u0026#34;Total Contribution\u0026#34;) #histograph after delete outliners\r boxplot(don$TGiving,horizontal=TRUE,xlab=\u0026#34;Total Contribution\u0026#34;) #boxplot with outliners\r boxplot(don$TGiving,outline=FALSE,horizontal=TRUE,xlab=\u0026#34;Total Contribution\u0026#34;) #boxplot without outliners\r dev.copy(png,\u0026#39;alumni_contributionplot.png\u0026#39;,width = 800, height = 500)\rdev.off()\rddd=don[don$TGiving\u0026gt;=30000,] #seeing only total giving greater than 30K\r ddd\rddd1=ddd[,c(1:5,12)] #display colum from 1 to 5 and column 12\r ddd1\rddd1[order(ddd1$TGiving,decreasing=TRUE),] #display with decreasing\r #-----------------\r plot.new()\rpar(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2))\rboxplot(TGiving~Class.Year,data=don,outline=FALSE, xlab=\u0026#34;year\u0026#34;)\rboxplot(TGiving~Gender,data=don,outline=FALSE, xlab=\u0026#34;sex\u0026#34;)\rboxplot(TGiving~Marital.Status,data=don,outline=FALSE,xlab=\u0026#34;Marital status\u0026#34;)\rboxplot(TGiving~AttendenceEvent,data=don,outline=FALSE,xlab=\u0026#34;Attend event or not\u0026#34;)\rdev.copy(png,\u0026#39;alumni_distribution_boxplot.png\u0026#39;,width = 800, height = 500)\rdev.off()\rplot.new()\r#-----------------\r t4=tapply(don$TGiving,don$Major,mean,na.rm=TRUE)\rt4\rt5=table(don$Major)\rt5\rt6=cbind(t4,t5)\rt7=t6[t6[,2]\u0026gt;10,]\rt7[order(t7[,1],decreasing=TRUE),]\rplot(barchart(t7[,1],col=\u0026#34;black\u0026#34;))\rdev.copy(png,\u0026#39;alumni_major_barplot.png\u0026#39;,width = 800, height = 500)\rdev.off()\r#-----------------\r plot.new()\rt4=tapply(don$TGiving,don$Next.Degree,mean,na.rm=TRUE)\rt4\rt5=table(don$Next.Degree)\rt5\rt6=cbind(t4,t5)\rt7=t6[t6[,2]\u0026gt;10,]\rt7[order(t7[,1],decreasing=TRUE),]\rplot(barchart(t7[,1],col=\u0026#34;black\u0026#34;))\rdev.copy(png,\u0026#39;alumni_degree_barplot.png\u0026#39;,width = 800, height = 500)\rdev.off()\r#-----------------\r plot.new()\rdensityplot(~TGiving|factor(Class.Year),data=don[don$TGiving\u0026lt;=1000,][don[don$TGiving\u0026lt;=1000,]$TGiving\u0026amp;gt;0,],plot.points=FALSE,col=\u0026#34;black\u0026#34;)\rdev.copy(png,\u0026#39;alumni_year_densityplot.png\u0026#39;,width = 800, height = 500)\rdev.off()\rt11=tapply(don$TGiving,don$Class.Year,FUN=sum,na.rm=TRUE)\rt11\r#-----------------\r plot.new()\rpar(mfrow=c(1,1))\rbarplot(t11,ylab=\u0026#34;Average Donation\u0026#34;)\rdev.copy(png,\u0026#39;alumni_year_barplot.png\u0026#39;,width = 800, height = 500)\rdev.off()\r#-----------------\r plot.new()\rpar(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2))\rbarchart(tapply(don$FY04Giving,don$Class.Year,FUN=sum,\rna.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=\u0026#34;black\u0026#34;, main=\u0026#34;2004\u0026#34;)\rbarchart(tapply(don$FY03Giving,don$Class.Year,FUN=sum,\rna.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=\u0026#34;black\u0026#34;, main=\u0026#34;2003\u0026#34;)\rbarchart(tapply(don$FY02Giving,don$Class.Year,FUN=sum,\rna.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=\u0026#34;black\u0026#34;, main=\u0026#34;2002\u0026#34;)\rbarchart(tapply(don$FY01Giving,don$Class.Year,FUN=sum,\rna.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=\u0026#34;black\u0026#34;, main=\u0026#34;2001\u0026#34;)\rbarchart(tapply(don$FY00Giving,don$Class.Year,FUN=sum,\rna.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=\u0026#34;black\u0026#34;, main=\u0026#34;2000\u0026#34;)\r#same plot but with par\r #-----------------\r plot.new()\rpar(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(3,2),bg=\u0026#34;white\u0026#34;)\rbarplot(tapply(don$FY04Giving,don$Class.Year,FUN=sum,\rna.rm=TRUE),ylim=c(0,225000),col=\u0026#34;black\u0026#34;, main=\u0026#34;2004\u0026#34;)\rbarplot(tapply(don$FY03Giving,don$Class.Year,FUN=sum,\rna.rm=TRUE),ylim=c(0,225000),col=\u0026#34;black\u0026#34;, main=\u0026#34;2003\u0026#34;)\rbarplot(tapply(don$FY02Giving,don$Class.Year,FUN=sum,\rna.rm=TRUE),ylim=c(0,225000),col=\u0026#34;black\u0026#34;, main=\u0026#34;2002\u0026#34;)\rbarplot(tapply(don$FY01Giving,don$Class.Year,FUN=sum,\rna.rm=TRUE),ylim=c(0,225000),col=\u0026#34;black\u0026#34;, main=\u0026#34;2001\u0026#34;)\rbarplot(tapply(don$FY00Giving,don$Class.Year,FUN=sum,\rna.rm=TRUE),ylim=c(0,225000),col=\u0026#34;black\u0026#34;, main=\u0026#34;2000\u0026#34;)\rdev.copy(png,\u0026#39;alumni_annual_barplot.png\u0026#39;,width = 500, height = 800)\rdev.off()\r#-----------------\r plot.new()\rpar(mfrow=c(1,1))\rdon$TGivingIND=cut(don$TGiving,breaks=c(-1,0.5,10000000),labels=FALSE)-1\rmean(don$TGivingIND)\rt5=table(don$TGivingIND,don$Class.Year)\rt5\rbarplot(t5,beside=TRUE)\rmosaicplot(factor(don$Class.Year)~factor(don$TGivingIND))\rt50=tapply(don$TGivingIND,don$Class.Year,FUN=mean,na.rm=TRUE)\rt50\rp3=barchart(t50,horizontal=FALSE,xlab=\u0026#34;Class Year\u0026#34;,col=\u0026#34;black\u0026#34;, main=\u0026#34;TGiving\u0026#34;)\rdon$FY04GivingIND=cut(don$FY04Giving,c(-1,0.5,10000000),labels=FALSE)-1\rt51=tapply(don$FY04GivingIND,don$Class.Year,FUN=mean,na.rm=TRUE)\rt51\rp4=barchart(t51,horizontal=FALSE,xlab=\u0026#34;Class Year\u0026#34;,col=\u0026#34;black\u0026#34;, main=\u0026#34;FY04Giving\u0026#34;)\rgrid.arrange(p3, p4, ncol = 2)\rdev.copy(png,\u0026#39;alumni_annual_barplotfreq.png\u0026#39;,width = 800, height = 300)\rdev.off()\r","date":1570548645,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570548645,"objectID":"3c90810cb87907f9aaf6175cef70015e","permalink":"/post/2019-10-08-alumni-contribution/","publishdate":"2019-10-08T23:30:45+08:00","relpermalink":"/post/2019-10-08-alumni-contribution/","section":"post","summary":"This is a summary of example 2 in Chapter 2 of the book “Data Mining and Business Analytics with R”. The post keeps the original code with some polishing syntax for better plotting, particularly using ggplot2 package and gridExtra.\nCode and Data are saved in Github link\nlibrary(lattice)\rlibrary(ggplot2)\r#Using multiple plot function when using with ggplot\r #source(\u0026#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/genericfunctions/multiplot.R\u0026#34;)\r #----1. Data\r don \u0026lt;- read.csv(\u0026#34;contribution.csv\u0026#34;)\r#or read directly from the web\r #don \u0026lt;- read.","tags":["R","Plot"],"title":"Alumni Contributions","type":"post"},{"authors":null,"categories":null,"content":"Earned Value Management (EVM) is a widely used method, if not to say, a imperative method in project management. Without the EVM, the Project Managers and stakeholders (PM) cannot track and monitor the Progress and Schedule, cannot understand the causes of problems and delays.\nIn a large scale engineering project, Contractors shall have an excellent and FULL TIME experienced Planners who can confidently perform EVM analysis and report to the PM and the project team weekly and monthly. The analysis report shall includes\n Updated Project Schedule with actual progress; Comparison between the Actual Progress vs Baselines; Critical Path Analysis (CPA) and identification of factors contributing to especially delays; The EVM.  It seems that creating the graph like the one shown above is EASY. ^_^ Nope, it is not easy at all, but if it is not difficult if Planners understand the statistics and a smart way to work with compiling data.\nThis article presents 2 different ways to generate the EVM graphs.\nEVM for Consultancy Project using MS Project and R #Coded by Nam Le (namlt@prontonmail.ch)\r library(scales)\rlibrary(reshape2)\rlibrary(ggplot2)\r#----------DATA-----------------\r data \u0026lt;- read.csv(file=\u0026#34;Scurve.csv\u0026#34;, header=TRUE, sep=\u0026#34;,\u0026#34;)\rk=data.frame(data)\r# -----------------------------This needs to be modified whenever you plot\r plot.new()\rpar(mar=c(4, 4, 4, 4), bg=\u0026#34;ivory\u0026#34;)\rweek= length(data[!is.na(data[,3]),3]) #the lastest week that EV value is available. Or you can simply put the number of week in instead of using length(data[!is.na(data[,3]),3]). Here 3 is the column.\r scalefactor=100\rtrucy=100\rticktime=35 #Set the Project Week at which you want to limit the X-axis\r varrange=c(-30,30)\r#Posion of texbox (PV, EV, and VAR in the graph)\r htextbox=c(week-5,week)\rvtextbox=20\r# --------------------------------\r # ----------------------------\r #Plot the baseline\r plot(data$BS01[1:ticktime],pch=4,axes=FALSE,ylim=c(0,trucy),ylab=\u0026#34;\u0026#34;, xlab=\u0026#34;\u0026#34;,col=\u0026#34;blue\u0026#34;,type=\u0026#34;o\u0026#34;,lwd=1,lty=1, main =paste(\u0026#34;S-curve of week\u0026#34;, week))\raxis(2, ylim=c(0,trucy),col=\u0026#34;darkblue\u0026#34;,las=1)\rmtext(expression(paste(\u0026#34;Cummulative percentage (%)\u0026#34;)),side=2,line=2.2, adj = 0.5 )\raxis(1,pretty(range(data$Week),30))\rmtext(expression(paste(\u0026#34;Project Week\u0026#34;)),side=1,col=\u0026#34;black\u0026#34;,line=2.2)\rbox()\r#plot the EV curve\r par(new=TRUE)\rplot(data$EV[1:ticktime],pch=18,axes=FALSE,ylim=c(0,trucy),ylab=\u0026#34;\u0026#34;, xlab=\u0026#34;\u0026#34;,col=\u0026#34;red\u0026#34;,type=\u0026#34;o\u0026#34;,lwd=1,lty=2)\r#adding stick at the actual week\r abline(v=week, col=\u0026#34;darkviolet\u0026#34;,lty=3)\rpar(new=TRUE)\r#library(gplots)\r library(astro) #this package is for astronomy but it has some fantastic function for plotting. Here I use the textbox function\r textbox(htextbox, vtextbox, textlist=c(paste(\u0026#34;PV =\u0026#34;,format(round(data$BS01[week],2)),\u0026#34;EV =\u0026#34;,format(round(data$EV[week],2)),\u0026#34;VAR=\u0026#34;,format(round(data$EV[week]-data$BS01[week],2)))), justify=\u0026#39;f\u0026#39;, cex=0.7,col=\u0026#34;purple\u0026#34;, font=2, border=\u0026#34;green\u0026#34;, margin=-0.025,adj=0,box=1,fill=\u0026#34;aliceblue\u0026#34;)\r#alternatively, we can also use legend function for the same purpose, but I believe textbox function gives better look.\r #legend(30, 50, c(paste(\u0026#34;PV:\u0026#34;),format(round(data$BS01[week],2)),paste(\u0026#34;EV:\u0026#34;),format(round(data$EV[week],2)), paste(\u0026#34;VAR:\u0026#34;),format(round(data$BS01[week]-data$EV[week],2))), col=c(\u0026#34;blue\u0026#34;,\u0026#34;red\u0026#34;,\u0026#34;green\u0026#34;),cex = 0.6)\r #plot the VAR curve using\r par(new=TRUE)\rplot((data$EV[1:ticktime]-data$BS01[1:ticktime]),pch=1,axes=FALSE,ylim=varrange,ylab=\u0026#34;\u0026#34;, xlab=\u0026#34;\u0026#34;,col=\u0026#34;cadetblue4\u0026#34;,type=\u0026#34;o\u0026#34;,lwd=1,lty=3)\raxis(4, ylim=varrange,col=\u0026#34;darkblue\u0026#34;,las=1)\rmtext(expression(paste(\u0026#34;Variance (%)\u0026#34;)),side=4,line=2.2, adj = 0.5 )\rabline(h=0, col=\u0026#34;darkviolet\u0026#34;,lty=3)\r#plot the AC curve: This curve is added only for internal monitoring purpose. Shall not give this to the other stakeholders.\r par(new=TRUE)\rplot(data$AC[1:ticktime],pch=2,axes=FALSE,ylim=c(0,trucy),ylab=\u0026#34;\u0026#34;, xlab=\u0026#34;\u0026#34;,col=\u0026#34;violetred3\u0026#34;,type=\u0026#34;o\u0026#34;,lwd=1,lty=1)\r#axis(4, ylim=varrange,col=\u0026#34;darkblue\u0026#34;,las=1)\r #mtext(expression(paste(\u0026#34;Variance (%)\u0026#34;)),side=4,line=2.2, adj = 0.5 )\r #abline(h=0, col=\u0026#34;darkviolet\u0026#34;,lty=3)\r #adding Planned Value, Earned Value, and Variance\r #adding legend\r legend(\u0026#34;topleft\u0026#34;, c(\u0026#34;PV\u0026#34;,\u0026#34;EV\u0026#34;, \u0026#34;VAR\u0026#34;, \u0026#34;AC\u0026#34;), text.col\r=\u0026#34;red\u0026#34;, border = \u0026#34;white\u0026#34;,box.lwd = 1,bg=\u0026#34;aliceblue\u0026#34;,lwd = 1,pch=c(4,18,1,2),lty =c(1,2,3,1), col=c(\u0026#34;blue\u0026#34;,\u0026#34;red\u0026#34;,\u0026#34;cadetblue4\u0026#34;,\u0026#34;violetred3\u0026#34;),inset = .05,cex=0.8)\r#save png file\r dev.copy(png,\u0026#39;evm.png\u0026#39;,width = 800, height = 500)\rdev.off()\r#-----THE END------------\r ","date":1570462245,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570462245,"objectID":"a31032016f359a62c3a0f65bbbcd2902","permalink":"/post/2019-10-07-evm/","publishdate":"2019-10-07T23:30:45+08:00","relpermalink":"/post/2019-10-07-evm/","section":"post","summary":"Earned Value Management (EVM) is a widely used method, if not to say, a imperative method in project management. Without the EVM, the Project Managers and stakeholders (PM) cannot track and monitor the Progress and Schedule, cannot understand the causes of problems and delays.\nIn a large scale engineering project, Contractors shall have an excellent and FULL TIME experienced Planners who can confidently perform EVM analysis and report to the PM and the project team weekly and monthly.","tags":["EVM","Project Management","Large Scale Construction Project"],"title":"Earned Value Management","type":"post"},{"authors":null,"categories":null,"content":"This post describes a generic approach on the computation of Net Present Value (NPV) and Life Cycle Cost estimation (LCC) for Preventive Intervention (PI) on pumps (e.g. centrifugal pumps) when decision makers are uncertain on, or do not have a rich set of data on failure of pump components.\nIdeally, failure data on pump components should be recorded in time-series fashion that allows analyst to investigate on the frequency of failure and come up with a prediction. The prediction is powerful in view of generating budget for purchasing spare parts for not only one pump station but also for the entire network of pump stations. This is so-called Integrated Asset Management approach that should be at the center of an organization who has a long term view on how to sustain and prolong their assets.\nHowever, in many practical situations, failure data is limited or recorded inappropriately that does not become a useful set of information for analytic. In such a circumstance, decisions on whether to replace/rehabilitate pumps should be dependent on various uncertainty factors such as efficiency, assumption on average decaying rate of the assets, and most importantly the energy consumption vs the production produce.\nThis article presents a simple yet useful model for managers to make decisions on replacement of pumps if they are more or less aware of deficiency, low efficiency, and high ratio of energy and production over time.\nAside from the above, it is also worth to consider replacement of pumps when the two following aspects are existed.\n Asset obsolescence; Technological innovation.  There two aspects should be viewed in a combination. Asset obsolescence could be a result of technological innovation or changes in standards and requirements. Suffice it to state that pumps of a station were designed/installed more than a decade ago, nowadays, pumps are designed and manufactured to be more reliable and durable, whilst the capital cost is significant lower than that in the past.\nThe model Total Cost (TC) incurred in a period of T years is defined as the summation of Capital Cost (C) and Operation cost (O). It is note that operational cost includes routine maintenance cost.\n$$ TC(T) = \\sum_{i=1}^{I} \\sum_{t=1}^{T}\\frac{C_{i,t}+O_{i,t}\\times \\epsilon_{i,t}^{new}\\times u_{i,t}+(1-\\delta_{i,t})\\times O_{i,t}\\epsilon_{i,t}^{old}\\times u_{i,t}}{(1+\\rho)^t}$$\nIn the equation, $\\epsilon_{i,t}$, $u_{i,t}$, and $\\delta_{i,t}$ are the decreasing rate of efficiency, utilization, and decision variable of pump i in year t, respectively.\nExample A pump station has 6 booster pumps with the same configuration of 500 horsepower. The station has been in commission for more than 10 years and there has been a progressive degradation process, though it has not been captured appropriately. Sufficient failure data at component level is not available.\nTests have been conducted to measure the flow and the power rating that allows to come up efficiency of pump. However, due to non-optimal pipe configuration, the obtained efficiencies are also subjected to uncertainties.\nUnder such circumstance, decision makers have to make decisions under uncertainties.\n Discount factor is 8.5% annually; Capital cost for buying/installing a new pump of 500 horsepower would cost 5 millions Peso (~10,000 usd); Efficiency of existing pumps are about 5% less than that of the original design; Pumps are assumed to operate at 98% utilization level; Price of 1 KW is 6.5 Peso; There are 2 Intervention Strategies (IS) to be considerred. Herein refer to TC1 and TC2, respectively. TC1 is to replace existing pump with a new one in a step of 1 year. TC2 is Do Nothing, i.e., to keep the existing pumps as they are.  Source Code #this subroutine is used to estimate the cashflow of an investment for pump stations\r Npumps = 6 #total number of pumps\r TPeriod=10 #this infers 5 years\r TC1\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #Option 1\r TC2\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #Option 2\r CAPEX\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #Captical investment cost\r OPEX\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #Operational Cost\r delta\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #decrease in efficiency\r epsilonold\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #decrease in efficiency\r epsilonnew\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #decrease in efficiency\r u\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #ultilization\r rho=0.085 #discount factor\r pricee=6.5\rhoursepower=500\rutilization=0.98\rNewpumpcost=5000000\r#Option 1 - Stagging investment for pump\r for (t in 1: TPeriod){\rif (t\u0026lt;2){\rfor (i in 1:Npumps){\repsilonold[t,i]\u0026lt;-1.05\r}\r} else {\rfor (i in 1:Npumps){\repsilonold[t,i]\u0026lt;-epsilonold[t-1,i]*1.05\r}\r}\r}\rfor (t in 1:TPeriod){\rfor (i in 1:Npumps){\rdelta[,]\u0026lt;-0\rdelta[1,1]\u0026lt;-1\repsilonnew[1,1]\u0026lt;-1\rdelta[2,2]\u0026lt;-1\repsilonnew[2,2]\u0026lt;-1\rdelta[3,3]\u0026lt;-1\repsilonnew[3,3]\u0026lt;-1\rdelta[4,4]\u0026lt;-1\repsilonnew[4,4]\u0026lt;-1\rdelta[5,5]\u0026lt;-1\repsilonnew[5,5]\u0026lt;-1\rdelta[6,6]\u0026lt;-1\repsilonnew[6,6]\u0026lt;-1\rCAPEX[t,i]\u0026lt;-Newpumpcost #peso\r u[t,i]\u0026lt;-utilization\rOPEX[t,i]\u0026lt;- pricee*hoursepower*365*24*0.746*u[t,i] #peso\r }\r}\rfor (t in 1: TPeriod){\rfor (i in 1:Npumps){\rif (delta[t,i]==1){\repsilonnew[t,i]\u0026lt;-1\r} else if (i==1 \u0026amp;amp; t==1){\repsilonnew[t,i]\u0026lt;-1\r} else if (t\u0026amp;gt;1){\repsilonnew[t,i]\u0026lt;-epsilonnew[t-1,i]*1\r}\r}\r}\repsilonnew[is.na(epsilonnew)] \u0026lt;- 0\r#redefine the value of epsilonold based on new value of epsilonnew\r for (t in 1: TPeriod){\rfor (i in 1:Npumps){\rif (epsilonnew[t,i]\u0026lt;1){\repsilonold[t,i]\u0026lt;-epsilonold[t,i]\r} else if (epsilonnew[t,i]==1){\repsilonold[t,i]\u0026lt;-0\r}\r}\r}\rfor (i in 1:Npumps){\rfor (t in 1:TPeriod){\rTC1[t,i]\u0026lt;-(((CAPEX[t,i]*delta[t,i]+OPEX[t,i]*epsilonnew[t,i]))+(1-delta[t,i])*OPEX[t,i]*epsilonold[t,i])/(1+rho)**t\r}\r}\rcat(\u0026#34;Replacement of pumps \\n\u0026#34;)\rprint(TC1)\r#stop(\u0026#34;dd\u0026#34;)\r #Option 2 - Do Nothing\r for (t in 1:TPeriod){\rfor (i in 1:Npumps){\rdelta[,]\u0026lt;-0\rCAPEX[t,i]\u0026lt;-0 #peso\r u[t,i]\u0026lt;-utilization\repsilonnew[t,i]\u0026lt;-1.0\rOPEX[t,i]\u0026lt;- pricee*hoursepower*365*24*0.746*u[t,i] #peso\r }\r}\rfor (t in 1: TPeriod){\rif (t\u0026lt;2){\rfor (i in 1:Npumps){\repsilonold[t,i]\u0026lt;-1.05\r}\r} else {\rfor (i in 1:Npumps){\repsilonold[t,i]\u0026lt;-epsilonold[t-1,i]*1.05\r}\r}\r}\rfor (i in 1:Npumps){\rfor (t in 1:TPeriod){\rTC2[t,i]\u0026lt;-(((CAPEX[t,i]+OPEX[t,i]*epsilonnew[t,i])*delta[t,i])+(1-delta[t,i])*OPEX[t,i]*epsilonold[t,i])/(1+rho)**t\r}\r}\rcat(\u0026#34;Do Nothing \\n\u0026#34;)\rprint(TC2)\rcat(\u0026#34;The difference of investment \\n\u0026#34;)\rprint(TC1-TC2)\r#plot the graph for comparison\r library(ggplot2)\rtime=c(1:TPeriod)\rx\u0026lt;-data.frame(dat=TC1[,1],IS=rep(\u0026#34;TC1\u0026#34;))\ry\u0026lt;-data.frame(dat=TC2[,1],IS=rep(\u0026#34;TC2\u0026#34;))\rx\u0026lt;-cbind(time,x)\ry\u0026lt;-cbind(time,y)\rxy\u0026lt;- rbind(x, y)\rggplot(xy, aes(fill=IS, y=dat, x=factor(time))) + geom_bar(position=\u0026#34;dodge\u0026#34;, stat=\u0026#34;identity\u0026#34;)\rstop(\u0026#34;\u0026#34;)\rTC1\u0026lt;-data.frame(TC1,IS=rep(\u0026#34;TC1\u0026#34;))\rTC2\u0026lt;-data.frame(TC2,IS=rep(\u0026#34;TC2\u0026#34;))\r#TC2\u0026lt;-data.frame(TC2=TC2[,1])\r #TC1\u0026lt;-data.frame(TC1)\r #TC2\u0026lt;-data.frame(TC2)\r TC1\u0026lt;-cbind(time,TC1)\rTC2\u0026lt;-cbind(time,TC2)\rlibrary(reshape)\rmdataTC1 \u0026lt;- melt(TC1,id=c(\u0026#34;time\u0026#34;,\u0026#34;IS\u0026#34;))\rmdataTC2 \u0026lt;- melt(TC2,id=c(\u0026#34;time\u0026#34;,\u0026#34;IS\u0026#34;))\r#joint the two data\r total \u0026lt;- rbind(mdataTC1, mdataTC2)\rggplot(total, aes(fill=IS, y=value, x=factor(time)),variable=X1) + geom_bar(position=\u0026#34;dodge\u0026#34;, stat=\u0026#34;identity\u0026#34;)\r#ggplot(total, aes(fill=variable, y=value, x=factor(time)),variable=X1) + geom_bar(position=\u0026#34;fill\u0026#34;, stat=\u0026#34;identity\u0026#34;)\r #ggplot(total, aes(y=value, x=variable, color=IS, fill=IS)) + geom_bar( stat=\u0026#34;identity\u0026#34;) + facet_wrap(~factor(time))\r Results Estimation results are shown in following tables\n source(\u0026lsquo;C:/Dropbox/workspace/RProjects/PlantAudit/PAG/tc.R\u0026rsquo;) Replacement of pumps\n [,1] [,2] [,3] [,4] [,5] [,6]\r[1,] 23791565 20142433 20142433 20142433 20142433 20142433\r[2,] 17680433 21927709 19492677 19492677 19492677 19492677\r[3,] 16295330 16295330 20209870 18863881 18863881 18863881\r[4,] 15018737 15018737 15018737 18626609 18255369 18255369\r[5,] 13842154 13842154 13842154 13842154 17167381 17666486\r[6,] 12757746 12757746 12757746 12757746 12757746 15822471\r[7,] 11758291 11758291 11758291 11758291 11758291 11758291\r[8,] 10837135 10837135 10837135 10837135 10837135 10837135\r[9,] 9988142 9988142 9988142 9988142 9988142 9988142\r[10,] 9205661 9205661 9205661 9205661 9205661 9205661\rDo Nothing\r[,1] [,2] [,3] [,4] [,5] [,6]\r[1,] 20142433 20142433 20142433 20142433 20142433 20142433\r[2,] 19492677 19492677 19492677 19492677 19492677 19492677\r[3,] 18863881 18863881 18863881 18863881 18863881 18863881\r[4,] 18255369 18255369 18255369 18255369 18255369 18255369\r[5,] 17666486 17666486 17666486 17666486 17666486 17666486\r[6,] 17096599 17096599 17096599 17096599 17096599 17096599\r[7,] 16545096 16545096 16545096 16545096 16545096 16545096\r[8,] 16011383 16011383 16011383 16011383 16011383 16011383\r[9,] 15494887 15494887 15494887 15494887 15494887 15494887\r[10,] 14995052 14995052 14995052 14995052 14995052 14995052\rThe difference of investment\r[,1] [,2] [,3] [,4] [,5] [,6]\r[1,] 3649131 0 0 0.0 0.0 0\r[2,] -1812244 2435032 0 0.0 0.0 0\r[3,] -2568551 -2568551 1345989 0.0 0.0 0\r[4,] -3236632 -3236632 -3236632 371239.7 0.0 0\r[5,] -3824332 -3824332 -3824332 -3824332.0 -499104.8 0\r[6,] -4338854 -4338854 -4338854 -4338853.7 -4338853.7 -1274128\r[7,] -4786805 -4786805 -4786805 -4786805.2 -4786805.2 -4786805\r[8,] -5174249 -5174249 -5174249 -5174248.9 -5174248.9 -5174249\r[9,] -5506745 -5506745 -5506745 -5506744.7 -5506744.7 -5506745\r[10,] -5789391 -5789391 -5789391 -5789390.9 -5789390.9 -5789391\rAs can be seen from the table of difference, if pump 1 is to be replaced (TC1) now (year 1), the Owner needs to spend 5 millions Peso for the pump, plus the energy cost for that year, the total cost in year 1 would be 23,791,565 Peso. Meanwhile, if pump 1 is to follow TC2 (Do Nothing), there is a cost of 20,142,433 Peso, which is basically the energy cost. The difference between TC1 and TC2 in year 1 is 3,649,131 Peso, meaning the Owner will spend more money in year 1. In other words, there is a negative cash flow in the first year.\nHowever, from 2nd year onward, there is a decreasing in energy consumption, this is thanks to the higher efficiency pump that consume less energy. As a result, there is a saving of -1,812,244 Peso. This value has been discounted and it is the Net Present Value of the OPEX incurred in year two. This difference can be notably seen in the following graph. This graph clearly shows that more energy in following years will be saved if the pump 1 is to be replaced now. It can also be dictated that the amount of cost associated with the saving in energy will compensate the CAPEX within 2 years of investment.\n","date":1569771045,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569771045,"objectID":"2e84fd1494e8b81846aaea0cc1243cb5","permalink":"/post/2019-09-29-pump-efficiency/","publishdate":"2019-09-29T23:30:45+08:00","relpermalink":"/post/2019-09-29-pump-efficiency/","section":"post","summary":"This post describes a generic approach on the computation of Net Present Value (NPV) and Life Cycle Cost estimation (LCC) for Preventive Intervention (PI) on pumps (e.g. centrifugal pumps) when decision makers are uncertain on, or do not have a rich set of data on failure of pump components.\nIdeally, failure data on pump components should be recorded in time-series fashion that allows analyst to investigate on the frequency of failure and come up with a prediction.","tags":["Pump efficiency","Pump Replacement","Asset Management"],"title":"Return on Investment (ROI) for Intervention of Pumps under uncertainties","type":"post"},{"authors":null,"categories":null,"content":"new header new 2nd header new level What if our Client provides us with a number of poorly recorded databases with thousands of records in different Excel formats? not only that, within each Excel file, there are hundreds of worksheets with in-homogeneous structures –F.. Excel ^_^, but that is the reality, particularly when we have to work with Clients whose business are not much involved with TECH.\nWhat if our Client provides us with a number of poorly recorded databases with thousands of records in different Excel formats? not only that, within each Excel file, there are hundreds of worksheets with in-homogeneous structures –F\u0026hellip;. Excel ^_^, but that is the reality, particularly when we have to work with Clients whose business are not much involved with TECH.\nLet start with time series data that is saved in hundreds of excel worksheets, examples are\n Production and consumption data of a single day. Within a day, volumes of production and consumption (e.g. water and energy of a water treatment plant or pump station, number of X and Y produced within a single hours); Hydrology data such as rainfall and runoff and other associated parameters etc  Ideally, those data should be recorded using relational database structured system such as MySQL or PostgreSQL. However, by default in many organizations, data is recorded in excel file. Everyday, owner of the file just multiply/copy the same worksheet of previous day and repeat the same work. This is OK for him/her but definitely not OK for us, the analyst :).\nThis post describes a step by step instruction on how to deal with this issue.\nExamples Data Example data is with two excel files name data1 and data2. These two files have their structure identical as shown in the followings\ndata1\rworksheet1\rdate from to total_pro_hour total_power_hour ratio\r01-Jan-17 12:00 AM1:00 AM 0.7700000 95 123.38\r01-Jan-17 1:00 AM 2:00 AM 0.7400000 86 116.22\r01-Jan-17 2:00 AM 3:00 AM 0.6200000 73 117.74\r01-Jan-17 3:00 AM 4:00 AM 0.6100000 70 114.75\r01-Jan-17 4:00 AM 5:00 AM 0.5700000 62 108.77\r01-Jan-17 5:00 AM 6:00 AM 0.7300000 86 117.81\r01-Jan-17 6:00 AM 7:00 AM 0.6900000 70 101.45\r01-Jan-17 7:00 AM 8:00 AM 1.0100000 93 92.08\r01-Jan-17 8:00 AM 9:00 AM 0.9100000 116 127.47\r01-Jan-17 9:00 AM 10:00AM 1.0800000 120 111.11\r01-Jan-17 10:00 AM 11:00AM 1.0900000 122 111.93\r01-Jan-17 11:00 AM 12:00PM 1.0800000 119 110.19\r01-Jan-17 12:00 PM 1:00PM 1.1000000 117 106.36\r01-Jan-17 1:00 PM 2:00 PM 1.5420000 145 94.03\r01-Jan-17 2:00 PM 3:00 PM 0.9330000 136 145.77\r01-Jan-17 3:00 PM 4:00 PM 1.0520000 137 130.23\r01-Jan-17 4:00 PM 5:00 PM 0.9600000 153 159.38\r01-Jan-17 5:00 PM 6:00 PM 0.9910000 135 136.23\r01-Jan-17 6:00 PM 7:00 PM 1.0110000 146 144.41\r01-Jan-17 7:00 PM 8:00 PM 0.9320000 134 143.78\r01-Jan-17 8:00 PM 9:00 PM 0.9680000 133 137.40\r01-Jan-17 9:00 PM 10:00 PM 0.7110000 124 174.40\r01-Jan-17 10:00 PM 11:00 PM 0.7800000 72 92.31\r01-Jan-17 11:00 PM 12:00 AM 0.6100000 75 122.95\rworksheet 2\r02-Jan-17 12:00 AM 1:00 AM 0.5600000 48 85.71\r02-Jan-17 1:00 AM 2:00 AM 0.4420000 42 95.02\r02-Jan-17 2:00 AM 3:00 AM 0.4700000 40 85.11\r02-Jan-17 3:00 AM 4:00 AM 0.6980000 59 84.53\r02-Jan-17 4:00 AM 5:00 AM 0.8200000 86 104.88\r02-Jan-17 5:00 AM 6:00 AM 0.4700000 48 102.13\r02-Jan-17 6:00 AM 7:00 AM 1.0400000 121 116.35\r02-Jan-17 7:00 AM 8:00 AM 1.0800000 146 135.19\r02-Jan-17 8:00 AM 9:00 AM 1.0800000 122 112.96\r02-Jan-17 9:00 AM 10:00 AM 0.9600000 82 85.42\r02-Jan-17 10:00 AM 11:00 AM 0.9100000 73 80.22\r02-Jan-17 11:00 AM 12:00 PM 0.8500000 65 76.47\r02-Jan-17 12:00 PM 1:00 PM 0.7100000 57 80.28\r02-Jan-17 1:00 PM 2:00 PM 0.9690000 48 49.54\r02-Jan-17 2:00 PM 3:00 PM 0.8310000 65 78.22\r02-Jan-17 3:00 PM 4:00 PM 1.1290000 96 85.03\r02-Jan-17 4:00 PM 5:00 PM 1.2300000 109 88.62\r02-Jan-17 5:00 PM 6:00 PM 1.1210000 114 101.69\r02-Jan-17 6:00 PM 7:00 PM 0.9440000 110 116.53\r02-Jan-17 7:00 PM 8:00 PM 0.9790000 112 114.40\r02-Jan-17 8:00 PM 9:00 PM 1.0260000 108 105.26\r02-Jan-17 9:00 PM 10:00 PM 0.8710000 118 135.48\r02-Jan-17 10:00 PM 11:00 PM 0.8100000 99 122.22\r02-Jan-17 11:00 PM 12:00 AM 0.6800000 79 116.18\rThis excel file contains 2 worksheet “1” and “2” that record hourly production (ML) and energy consumption (KW) of a water pump station.\n column is the date; column 2 and 3 are hours; column 4 is production data in million liter (ML); column 5 is energy consumption data in KW column 6 is ratio between energy consumption and production, basically it is the division of column 5 and 4.  This kind of data is recorded hourly and there will be about 30 worksheets for one month. Let say you have 5 or 10 years production data saving in excel files like this and you need to combine all of them into a single frame for Business Analysis purpose. It will be a nightmare if you just copy and paste 🙂 terrible excel.\nAssumptions It is assumed that all worksheets in all excel files are identical in their structure. This is not a realistic assumption as excel users cannot be consistent with their data. Data in their excel files are\nMixed up with numeric and text even for the same attributes; Merging cells, adding new rows and columns that make them homogeneous. Solving such problem is not the objective of this post, but it is worth to mention that before we get a good set of data, we probably need to do some Coding in Visual Basic to standardize the excel worksheets, or we need to do a certain level of manual data compiling before we can run the code in R.\nWill cover how to standardize using the same sets of data in other post.\nCombining worksheets using Navicat and MySQL Why NAVICAT?\n–\u0026gt; Navicat offers a handy way to import data from excel files. It can import multiple worksheets in one single click into respective tables of MySQL. I find this feature superior than other open source SQL Client such PhPmyAdmin, Dbeaver, etc.\nHerein, I demonstrate some steps to import the example data files.\na. Create table\ncreate\rtable\r`1` (\rdate varchar(255),\r`from` varchar(255),\r`to` varchar(255),\rtotal_pro_hour varchar(255),\rtotal_power_hour varchar(255),\rratio varchar(255)\r);\rcreate table `2` as select * from `1`;\rcreate table `3` as select * from `1`;\rcreate table `4` as select * from `1`;\rcreate table `5` as select * from `1`;\rcreate table `6` as select * from `1`;\rcreate table `7` as select * from `1`;\rcreate table `8` as select * from `1`;\rcreate table `9` as select * from `1`;\rcreate table `10` as select * from `1`;\rcreate table `11` as select * from `1`;\rcreate table `12` as select * from `1`;\rcreate table `13` as select * from `1`;\rcreate table `14` as select * from `1`;\rcreate table `15` as select * from `1`;\rcreate table `16` as select * from `1`;\rcreate table `17` as select * from `1`;\rcreate table `18` as select * from `1`;\rcreate table `19` as select * from `1`;\rcreate table `20` as select * from `1`;\rcreate table `21` as select * from `1`;\rcreate table `22` as select * from `1`;\rcreate table `23` as select * from `1`;\rcreate table `24` as select * from `1`;\rcreate table `25` as select * from `1`;\rcreate table `26` as select * from `1`;\rcreate table `27` as select * from `1`;\rcreate table `28` as select * from `1`;\rcreate table `29` as select * from `1`;\rcreate table `30` as select * from `1`;\rThis SQL creates 30 tables name from 1 to 30 that correspond to each day in a month. Note that for February, there are 28 days but dont worry, we still use 30 or 31 worksheets as additional worksheets will be blank anyway and make no harm to the operation.\nb. Manual importing worksheets from the excel file to MySQL table using NaviCAT\n Select table 1 in MySQL database Right Click –\u0026gt; Import Wizard Select Excel file and Click Next Import the excel file  Now you have all tables you need in MySQL. However, Values of time shown in FROM and TO column have change from AM, PM to something else. For example 1900-01-02 should be 24. To solve this issue, we will use the following SQL syntax\nc. Rename tables\nRENAME TABLE\r`1` TO pat_2018_10_1,\r`2` TO pat_2018_10_2,\r`3` TO pat_2018_10_3,\r`4` TO pat_2018_10_4,\r`5` TO pat_2018_10_5,\r`6` TO pat_2018_10_6,\r`7` TO pat_2018_10_7,\r`8` TO pat_2018_10_8,\r`9` TO pat_2018_10_9,\r`10` TO pat_2018_10_10,\r`11` TO pat_2018_10_11,\r`12` TO pat_2018_10_12,\r`13` TO pat_2018_10_13,\r`14` TO pat_2018_10_14,\r`15` TO pat_2018_10_15,\r`16` TO pat_2018_10_16,\r`17` TO pat_2018_10_17,\r`18` TO pat_2018_10_18,\r`19` TO pat_2018_10_19,\r`20` TO pat_2018_10_20,\r`21` TO pat_2018_10_21,\r`22` TO pat_2018_10_22,\r`23` TO pat_2018_10_23,\r`24` TO pat_2018_10_24,\r`25` TO pat_2018_10_25,\r`26` TO pat_2018_10_26,\r`27` TO pat_2018_10_27,\r`28` TO pat_2018_10_28,\r`29` TO pat_2018_10_29,\r`30` TO pat_2018_10_30\r;\rHere we rename the table to whatever we need. By doing so, we can resue the Create Query to perform the same procedure for importing new table.\nd. Create Production table\ncreate\rtable\rproduction (\rdate varchar(255),\r`from` varchar(255),\r`to` varchar(255),\rtotal_pro_hour varchar(255),\rtotal_power_hour varchar(255),\rratio varchar(255)\r);\rThis code creates a production table.\ne. Import/Append all raw tables into one table – Production\nINSERT INTO production\rSELECT *\rFROM pat_2017_1_1;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_2;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_3;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_4;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_5;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_6;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_7;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_8;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_9;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_10;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_11;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_12;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_13;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_14;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_15;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_16;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_17;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_18;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_19;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_20;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_21;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_22;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_23;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_24;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_25;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_26;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_27;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_28;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_29;\rINSERT INTO production\rSELECT *\rFROM pat_2017_1_30;\rThis code imports all data from raw table into production table.\nf. Correct data, particularly with date and time\nDROP TABLE IF EXISTS productioncorrected;\rcreate table productioncorrected select * from production;\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;1900-01-01\u0026#39;,\u0026#39;24:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;1900-01-02 \u0026#39;,\u0026#39;\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;1900-01-02\u0026#39;,\u0026#39;00:00:00.000\u0026#39;);\r#####\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;1:00AM\u0026#39;,\u0026#39;01:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;2:00AM\u0026#39;,\u0026#39;02:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;3:00AM\u0026#39;,\u0026#39;03:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;4:00AM\u0026#39;,\u0026#39;04:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;5:00AM\u0026#39;,\u0026#39;05:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;6:00AM\u0026#39;,\u0026#39;06:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;7:00AM\u0026#39;,\u0026#39;07:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;8:00AM\u0026#39;,\u0026#39;08:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;9:00AM\u0026#39;,\u0026#39;09:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;10:00AM\u0026#39;,\u0026#39;10:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;11:00AM\u0026#39;,\u0026#39;11:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;12:00PM\u0026#39;,\u0026#39;12:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;01:00PM\u0026#39;,\u0026#39;13:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;02:00PM\u0026#39;,\u0026#39;14:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;03:00PM\u0026#39;,\u0026#39;15:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;04:00PM\u0026#39;,\u0026#39;16:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;05:00PM\u0026#39;,\u0026#39;17:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;06:00PM\u0026#39;,\u0026#39;18:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;07:00PM\u0026#39;,\u0026#39;19:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;08:00PM\u0026#39;,\u0026#39;20:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;09:00PM\u0026#39;,\u0026#39;21:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;10:00PM\u0026#39;,\u0026#39;22:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;11:00PM\u0026#39;,\u0026#39;23:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;12:00AM\u0026#39;,\u0026#39;00:00:00.000\u0026#39;);\r#####\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;1:00AM\u0026#39;,\u0026#39;01:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;2:00AM\u0026#39;,\u0026#39;02:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;3:00AM\u0026#39;,\u0026#39;03:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;4:00AM\u0026#39;,\u0026#39;04:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;5:00AM\u0026#39;,\u0026#39;05:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;6:00AM\u0026#39;,\u0026#39;06:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;7:00AM\u0026#39;,\u0026#39;07:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;8:00AM\u0026#39;,\u0026#39;08:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;9:00AM\u0026#39;,\u0026#39;09:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;10:00AM\u0026#39;,\u0026#39;10:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;11:00AM\u0026#39;,\u0026#39;11:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;12:00PM\u0026#39;,\u0026#39;12:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;01:00PM\u0026#39;,\u0026#39;13:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;02:00PM\u0026#39;,\u0026#39;14:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;03:00PM\u0026#39;,\u0026#39;15:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;04:00PM\u0026#39;,\u0026#39;16:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;05:00PM\u0026#39;,\u0026#39;17:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;06:00PM\u0026#39;,\u0026#39;18:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;07:00PM\u0026#39;,\u0026#39;19:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;08:00PM\u0026#39;,\u0026#39;20:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;09:00PM\u0026#39;,\u0026#39;21:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;10:00PM\u0026#39;,\u0026#39;22:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;11:00PM\u0026#39;,\u0026#39;23:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;12:00AM\u0026#39;,\u0026#39;24:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;101:00:00.000\u0026#39;,\u0026#39;11:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,\u0026#39;102:00:00.000\u0026#39;,\u0026#39;24:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;101:00:00.000\u0026#39;,\u0026#39;11:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,\u0026#39;102:00:00.000\u0026#39;,\u0026#39;24:00:00.000\u0026#39;);\rUPDATE productioncorrected SET productioncorrected.`to` = \u0026#34;24:00:00.000\u0026#34;\rWHERE productioncorrected.`from`= \u0026#34;23:00:00.000\u0026#34;;\rUPDATE productioncorrected SET productioncorrected.`to` = \u0026#34;11:00:00.000\u0026#34;\rWHERE productioncorrected.`from`= \u0026#34;10:00:00.000\u0026#34;;\rUPDATE productioncorrected SET productioncorrected.`from` = \u0026#34;11:00:00.000\u0026#34;\rWHERE productioncorrected.`to`= \u0026#34;12:00:00.000\u0026#34;;\rUPDATE productioncorrected SET productioncorrected.`from` = \u0026#34;24:00:00.000\u0026#34;\rWHERE productioncorrected.`to`= \u0026#34;01:00:00.000\u0026#34;;\rThis code is used to correct\n time data missing data outliner  g. Create analysis table\nDROP TABLE IF EXISTS analysis;\rCREATE TABLE analysis\rSelect *\rfrom\rproductioncorrected\rwhere productioncorrected.total_pro_hour \u0026gt;0\rand\rproductioncorrected.total_power_hour\u0026gt;0\rand\rproductioncorrected.total_power_hour NOT LIKE \u0026#34;-\u0026#34;\r;\rALTER TABLE analysis MODIFY COLUMN total_power_hour DOUBLE;\rALTER TABLE analysis MODIFY COLUMN total_pro_hour DOUBLE;\rALTER TABLE analysis MODIFY COLUMN ratio DOUBLE;\rAnalysis table is basically the same with production but with some correction\nh. Create Analysis Date table\nDROP TABLE IF EXISTS analysisdate;\rCREATE TABLE analysisdate\rSelect\r*\rfrom\ranalysis\rGROUP BY analysis.date;\rALTER TABLE analysisdate\rADD id int NOT NULL AUTO_INCREMENT PRIMARY KEY;\rThis SQL code will create Analysis Date table that basically summary hourly data into daily data.\n WHAO, after going through each of the above Query, my figures are tired already. You can further enhance SQL code to make the entire process faster, but still it is much slower than using below R code.\n Combining worksheets using R Below R codes are extracted from Github source\nThis code is used to combine multiple excel worksheets into one dataframe. Particularly useful when combining multiple worksheets of production data with each worksheet is a date of a month, and in each worksheet, data is saved in hourly basis.\nthe first method, using XlConnect.\n# this method has a limitation that XlConnect doesnt work well with excel files with dynamic links. When importing into R, it gives NA values.\r # to make sure that importing is perfect. It is advisable to disable all dynamic links in excel file by going to Data\r library(XLConnect)\r# load data file (excel files ended with cls, xlsc, or xlsm)\r datafile \u0026lt;- loadWorkbook(\u0026#34;data1.xlsx\u0026#34;) # This is a static worksheet, without any dynamic links\r # obtain sheet names\r worksheets \u0026lt;- getSheets(datafile)\rnames(worksheets) \u0026lt;- worksheets\r# dataframe\r worksheets_list \u0026lt;- lapply(worksheets, function(.sheet){readWorksheet(object=datafile, .sheet)})\r# limit worksheet_list to sheets with at least 1 dimension\r worksheets_list2 \u0026lt;- worksheets_list[sapply(worksheets_list, function(x) dim(x)[1]) \u0026amp;gt; 0]\r# code to read in each excel worksheet as individual dataframes\r for (i in 2:length(worksheets_list2)){assign(paste0(\u0026#34;df\u0026#34;, i), as.data.frame(worksheets_list2[i]))}\r# define function to clean data in each data frame (updated based on your data). You must define here carefully otherwise it will not work well with some certain type of data. The fastest way is only drop out missing values. Other value can be dealed with using query in MySQL\r cleaner \u0026lt;- function(df){\r# drop rows with missing values\r df \u0026lt;- df[rowSums(is.na(df)) == 0,]\r# remove serial comma from all variables\r # df[,-1] \u0026lt;- as.numeric(gsub(\u0026#34;,\u0026#34;, \u0026#34;\u0026#34;, as.matrix(df[,-1])))\r # create numeric version of year variable for graphing\r # df$Year \u0026lt;- as.numeric(substr(df$year, 1, 4))\r # return cleaned df\r return(df)\r}\r# clean sheets and create one data frame\r data1 \u0026lt;- do.call(rbind,lapply(names(worksheets_list2), function(x) cleaner(worksheets_list2[[x]])))\rcat(\u0026#34;Print out the data 1 frame \\n\u0026#34;)\rprint(data1)\r# Method is with readxl package. This is superior than the former one as readxl can handle excel files with dynamic links. This means it will retain values and ignore the links.\r # ----------------\r library(readxl)\rread_excel_allsheets \u0026lt;- function(filename, tibble = FALSE) {\r# I prefer straight data.frames\r # but if you like tidyverse tibbles (the default with read_excel)\r # then just pass tibble = TRUE\r sheets \u0026lt;- readxl::excel_sheets(filename)\rx \u0026lt;- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))\rif(!tibble) x \u0026lt;- lapply(x, as.data.frame)\rnames(x) \u0026lt;- sheets\rx\r}\r#start to read and write data into csv file\r worksheets \u0026lt;- read_excel_allsheets(\u0026#34;data1.xlsx\u0026#34;)\rsource(\u0026#34;cleaning.R\u0026#34;)\rfiledata \u0026lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]])))\rwrite.table(filedata, \u0026#34;myDF.csv\u0026#34;, sep = \u0026#34;,\u0026#34;, col.names = !file.exists(\u0026#34;myDF.csv\u0026#34;), row.names=FALSE, append = T)\rworksheets \u0026lt;- read_excel_allsheets(\u0026#34;data2.xlsm\u0026#34;)\rsource(\u0026#34;cleaning.R\u0026#34;)\rfiledata \u0026lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]])))\rwrite.table(filedata, \u0026#34;myDF.csv\u0026#34;, sep = \u0026#34;,\u0026#34;, col.names = !file.exists(\u0026#34;myDF.csv\u0026#34;), row.names=FALSE, append = T)\r#end\r #https://medium.com/@niharika.goel/merge-multiple-csv-excel-files-in-a-folder-using-r-e385d962a90a\r Combining multiple excel files using R I found the code from Niharika suits the purpose of this exercise. Kindly refer to her github site for original code.\nhttps://github.com/NiharikaGoel12/R-Playground\n Her readme file states This repository contains basic codes for R, which might be useful in day to day work, especially doing data analysis on large datasets in Excel or CSV.\n#Merge multiple Excel/CSV files in a folder\nConsider a case when you have multiple xlsx/csv files in a folder \u0026amp; you to merge them into one single file. Here, I have used lapply() which returns a list of the same length as i. And grepl() will check exact match between merge_file_name \u0026amp; existing file ‘i’. In this case, if the two files are same, we will ignore already created “merge file”.\nrbind() will combine data frame by rows and merge all the files.\n a. Combining CSV files\npath \u0026lt;- \u0026#34;sample-data/merge-files/csv\u0026#34;\rmerge_file_name \u0026lt;- \u0026#34;sample-data/merge-files/merged_file.csv\u0026#34;\rfilenames \u0026lt;- list.files(path= path, full.names=TRUE)\rAll \u0026lt;- lapply(filenames,function(filename){\rprint(paste(\u0026#34;Merging\u0026#34;,filename,sep = \u0026#34;\u0026#34;))\rread.csv(filename)\r})\rdf \u0026lt;- do.call(rbind.data.frame, All)\rwrite.csv(df,merge_file_name)\rb. Combining excel files\nlibrary(openxlsx)\rpath \u0026lt;- \u0026#34;sample-data/merge-files/xlsx\u0026#34;\rmerge_file_name \u0026lt;- \u0026#34;sample-data/merge-files/merged_file.xlsx\u0026#34;\rfilenames_list \u0026lt;- list.files(path= path, full.names=TRUE)\rAll \u0026lt;- lapply(filenames_list,function(filename){\rprint(paste(\u0026#34;Merging\u0026#34;,filename,sep = \u0026#34;\u0026#34;))\rread.xlsx(filename)\r})\rdf \u0026lt;- do.call(rbind.data.frame, All)\rwrite.xlsx(df,merge_file_name)\rc. Combining the example data\nWe can use the code in step a and b of this section. However, there is one draw back that we need to save our data first into csv or excel file. This is also a bit of time consuming. To avoid this, we can just simply write directly data into csv file as presented in the last part of section 4.\n# ----------------\r library(readxl)\rread_excel_allsheets \u0026lt;- function(filename, tibble = FALSE) {\r# I prefer straight data.frames\r # but if you like tidyverse tibbles (the default with read_excel)\r # then just pass tibble = TRUE\r sheets \u0026lt;- readxl::excel_sheets(filename)\rx \u0026lt;- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))\rif(!tibble) x \u0026lt;- lapply(x, as.data.frame)\rnames(x) \u0026lt;- sheets\rx\r}\r#start to read and write data into csv file\r worksheets \u0026lt;- read_excel_allsheets(\u0026#34;data1.xlsx\u0026#34;)\rsource(\u0026#34;cleaning.R\u0026#34;)\rfiledata \u0026lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]])))\rwrite.table(filedata, \u0026#34;myDF.csv\u0026#34;, sep = \u0026#34;,\u0026#34;, col.names = !file.exists(\u0026#34;myDF.csv\u0026#34;), row.names=FALSE, append = T)\rworksheets \u0026lt;- read_excel_allsheets(\u0026#34;data2.xlsm\u0026#34;)\rsource(\u0026#34;cleaning.R\u0026#34;)\rfiledata \u0026lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]])))\rwrite.table(filedata, \u0026#34;myDF.csv\u0026#34;, sep = \u0026#34;,\u0026#34;, col.names = !file.exists(\u0026#34;myDF.csv\u0026#34;), row.names=FALSE, append = T)\r#end\r The above code read 2 data files data1.xlsx and data2.xlsm, combining all worksheets of these two files and then write the data frame to a CSV file named myDF.csv. If we have more than 2 files to read and combine, we can just copy the code and paste it under and remember to change the source data file. We can also automate this process by making a loop, which will be presented in other post.\nOnce we have the combined CSV file, we can use NaviCAT to import this file to MySQL for further enhancement as presented in section 3. In the end, we can summary the production and energy consumption data in day or week that will be useful for visualization, correlation, and regression analysis.\nVisualization Tableu R library(DBI)\rlibrary(RODBC)\rlibrary(RMySQL)\rlibrary(xts)\rlibrary(ggplot2)\rlibrary(hydroTSM) #call the hydrology package for time series analysis\r #this code is used for energy audit of the plant based on production and power consumption\r dataproduction = dbConnect(MySQL(), user=\u0026#39;root\u0026#39;, password=\u0026#39;\u0026#39;, dbname=\u0026#39;exampledata\u0026#39;, host=\u0026#39;localhost\u0026#39;)\r#exampledata is the database stored in your MySQL server\r dbListTables(dataproduction)\rdbListFields(dataproduction, \u0026#39;analysisdate\u0026#39;)\rrs = dbSendQuery(dataproduction, \u0026#34;select * from analysisdate\u0026#34;)\rrs=dbFetch(rs, n = -1)\rdata=c(rs[1],rs[4],rs[5],rs[6])\ra=as.Date(data$date,\u0026#34;%Y-%m-%d\u0026#34;)\rproduction=data$total_pro_hour\rpower=data$total_power_hour\rratio=data$ratio\rdata=data.frame(a,production,power,ratio)\r#Correlation analysis\r hydropairs(data[,2:3])\r#Plot production data\r plot.new()\rp=ggplot(data=data,aes(a, production))+ geom_line(color = \u0026#34;#00AFBB\u0026#34;) + theme(axis.text.x = element_text(angle = 60,hjust=1))+theme_gray(base_size = 14)+ theme(plot.title = element_text(size=12))\rp\rmin \u0026lt;- as.Date(\u0026#34;2017-01-01\u0026#34;)\rmax \u0026lt;- as.Date(\u0026#34;2018-10-30\u0026#34;)\r#p + scale_x_date(limits = c(min, max))\r pp=p+scale_x_date(date_labels = \u0026#34;%Y\u0026#34;,date_breaks=\u0026#34;year\u0026#34;,limits = c(min, max))+labs(title = \u0026#34;Production hourly\u0026#34;,x=\u0026#34;Years\u0026#34;,y=\u0026#34;ML\u0026#34;)+scale_y_continuous(limits=c(0,2))#+ stat_smooth(method=\u0026#34;lm\u0026#34;)\r ggsave(\u0026#34;ch05_fig_energy_production.png\u0026#34;, plot = pp)\rprint(pp)\r#Plot power consumption data\r plot.new()\rq=ggplot(data=data,aes(a, power))+ geom_line(color = \u0026#34;#00AFBB\u0026#34;) + theme(axis.text.x = element_text(angle = 60,hjust=1))+theme_gray(base_size = 14)+ theme(plot.title = element_text(size=12))\r#q\r min \u0026lt;- as.Date(\u0026#34;2017-01-01\u0026#34;)\rmax \u0026lt;- as.Date(\u0026#34;2018-10-30\u0026#34;)\r#p + scale_x_date(limits = c(min, max))\r qq=q+scale_x_date(date_labels = \u0026#34;%Y\u0026#34;,date_breaks=\u0026#34;year\u0026#34;,limits = c(min, max))+labs(title = \u0026#34;Power hourly\u0026#34;,x=\u0026#34;Years\u0026#34;,y=\u0026#34;KW\u0026#34;)+scale_y_continuous(limits=c(0,250))#+ stat_smooth(method=\u0026#34;lm\u0026#34;)\r ggsave(\u0026#34;ch05_fig_energy_power.png\u0026#34;, plot = qq)\rprint(qq)\r#plot ratio\r plot.new()\rw=ggplot(data=data,aes(a, ratio))+ geom_line(color = \u0026#34;#00AFBB\u0026#34;) + theme(axis.text.x = element_text(angle = 60,hjust=1))+theme_gray(base_size = 14)+ theme(plot.title = element_text(size=12))\r#w\r min \u0026lt;- as.Date(\u0026#34;2017-01-01\u0026#34;)\rmax \u0026lt;- as.Date(\u0026#34;2018-10-30\u0026#34;)\r#p + scale_x_date(limits = c(min, max))\r ww=w+scale_x_date(date_labels = \u0026#34;%Y\u0026#34;,date_breaks=\u0026#34;year\u0026#34;,limits = c(min, max))+labs(title = \u0026#34;Ratio\u0026#34;,x=\u0026#34;Years\u0026#34;,y=\u0026#34;\u0026#34;)+scale_y_continuous(limits=c(0,250))#+ stat_smooth(method=\u0026#34;lm\u0026#34;)\r ggsave(\u0026#34;ch05_fig_energy_ratio.png\u0026#34;, plot = ww)\rprint(ww)\rdbDisconnect(dataproduction)\r","date":1568561445,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568561445,"objectID":"6fdc373de278252ecd468c27839f3f14","permalink":"/post/2019-09-15-excel-combine/","publishdate":"2019-09-15T23:30:45+08:00","relpermalink":"/post/2019-09-15-excel-combine/","section":"post","summary":"new header new 2nd header new level What if our Client provides us with a number of poorly recorded databases with thousands of records in different Excel formats? not only that, within each Excel file, there are hundreds of worksheets with in-homogeneous structures –F.. Excel ^_^, but that is the reality, particularly when we have to work with Clients whose business are not much involved with TECH.\nWhat if our Client provides us with a number of poorly recorded databases with thousands of records in different Excel formats?","tags":["excel","mutiple worksheet"],"title":"Excel worksheets combination","type":"post"},{"authors":["Juergen Hackl and Bryan T Adey and **Nam Lethanh**"],"categories":[],"content":"","date":1531872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531872000,"objectID":"6336823b86b03db2f61c09ccd745fb45","permalink":"/publication/hackl2018/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/publication/hackl2018/","section":"publication","summary":"Disruptive events, such as earthquakes, floods, and landslides, may disrupt the service provided by transportation networks on a vast scale, as their occurrence is likely to cause multiple objects to fail simultaneously. The restoration program following a disruptive event should restore service as much, and as fast, as possible. The estimation of risk due to natural hazards must take into consideration the resilience of the network, which requires estimating the restoration program as accurately as possible. In this article, a restoration model using simulated annealing is formulated to determine near‐optimal restoration programs following the occurrence of hazard events. The objective function of the model is to minimize the costs, taking into consideration the direct costs of executing the physical interventions, and the indirect costs that are being incurred due to the inadequate service being provided by the network. The constraints of the model are annual and total budget constraints, annual and total resource constraints, and the specification of the number and type of interventions to be executed within a given time period. The restoration model is demonstrated by using it to determine the near‐optimal restoration program for an example road network in Switzerland following the occurrence of an extreme flood event. The strengths and weaknesses of the restoration model are discussed, and an outlook for future work is given.","tags":["Weibull","Tunnel","Asset Management"],"title":"Determination of Near‐Optimal Restoration Programs for Transportation Networks Following Natural Hazard Events Using Simulated Annealing","type":"publication"},{"authors":["Nam Lethanh and Bryan T. Adey and Marcel Burkhalter"],"categories":[],"content":"","date":1520640000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520640000,"objectID":"64e7afb2e98876016d77dea953c322e4","permalink":"/publication/lethanh2018/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/publication/lethanh2018/","section":"publication","summary":"A road network consists of multiple objects that deteriorate over time with different speeds of deterioration. In order to provide an adequate level of service over time, these objects will eventually require interventions. As road managers are trying, in general, to maximize the benefit obtained from the road network, it is in their interest to determine intervention programs, which consist of the grouping of interventions in work zones. The determination of optimal intervention programs is relatively complicated when considering single objects, but it becomes even more so when considering multiple objects embedded within a network. The objects to be included in the work zones at each time interval depend on many factors, such as the interventions to be executed on the objects, the maximum allowable length of the work zones, the traffic configurations to be used in the work zones and the available financial resources. Although some initial research in this area has been conducted, none has determined the optimal set of work zones on large infrastructure networks in a geographical information system (GIS) framework, which is necessary in the world of modern infrastructure management. In the work presented in this paper, a GIS-based program was developed to determine optimal intervention programs for large infrastructure networks using a linear optimization model, which can be linked directly to a GIS. The model includes constraints on the amount of available resources, on the length of the work zone, and on the distance between two work zones. A constraint-constructing algorithm is used in order to set up the latter two constraints. The program is illustrated by determining the optimal set of work zones for an example road network similar to the one in the canton of Wallis, Switzerland, including more than 2,000 bridges, tunnels, and road sections.","tags":["Optimization","GIS","Workzone","Asset Management"],"title":"Determining an Optimal Set of Work Zones on Large Infrastructure Networks in a GIS Framework","type":"publication"},{"authors":["Nam Lethanh  and Juergen Hackl  and Bryan T. Adey"],"categories":[],"content":"","date":1500249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500249600,"objectID":"66baddc42ade320793f9301b43f0a475","permalink":"/publication/lethanh2017/","publishdate":"2017-07-17T00:00:00Z","relpermalink":"/publication/lethanh2017/","section":"publication","summary":"Many bridge management systems use Markov models to predict the future deterioration of structural elements. This information is subsequently used in the determination of optimal intervention strategies and intervention programs. The input for these Markov models often consists of the condition states of the elements and how they have changed over time. This input is used to estimate the probabilities of transition of an object from each possible condition state to each other possible condition state in one time period. A complication in using Markov models is that there are situations in which there is an inadequate amount of data to estimate the transition probabilities using traditional methods (e.g., due to the lack of recording past information so that it can be easily retrieved, or because it has been collected in an inconsistent or biased manner). In this paper, a methodology to estimate the transition probabilities is presented that uses proportional data obtained by mechanistic-empirical models of the deterioration process. A restricted least-squares optimization model is used to estimate the transition probabilities. The methodology is demonstrated by using it to estimate the transition probabilities for a reinforced concrete (RC) bridge element exposed to chloride-induced corrosion. The proportional data are generated by modeling the corrosion process using mechanistic-empirical models and Monte Carlo simulations.","tags":["Markov","BMS","Asset Management","Regression","Optimization"],"title":"Determination of Markov Transition Probabilities to be Used in Bridge Management from Mechanistic-Empirical Models","type":"publication"},{"authors":["Nam Lethanh and Bryan T. Adey"],"categories":[],"content":"","date":1482019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482019200,"objectID":"18a162934edd1f4d524f8c6d1cf14f1f","permalink":"/publication/lethanh2016g/","publishdate":"2016-12-18T00:00:00Z","relpermalink":"/publication/lethanh2016g/","section":"publication","summary":"In this paper, a real option approach to determine the optimal time to execute interventions on rail infrastructure, when it is not known for certain which intervention is to be executed, is presented (i.e. the optimal intervention window). Such an approach is useful in the management of rail infrastructure that belongs to a multi-national rail corridor where multiple railway organizations, ideally, should coordinate their maintenance interventions, years in advance, to minimize service disruptions. The approach is based on an adaptation of the Black and Scholes differential equation model used to value European call options in financial engineering. It is demonstrated by determining the optimal intervention window for infrastructure in a fictive rail corridor.","tags":["Weibull","Railway","Real Option","Asset Management"],"title":"A real option approach to determine optimal intervention windows for multi-national rail corridor","type":"publication"},{"authors":["Nam Lethanh and Craig Richmond and Bryan T. Adey"],"categories":[],"content":"","date":1473465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473465600,"objectID":"3eb4f586e356c9b76748d7d05c962fbb","permalink":"/publication/lethanh2016b/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/publication/lethanh2016b/","section":"publication","summary":"In the management of road networks, it is often desired to know the condition of individual road sections, which is approximated using the values of condition indicators. The values of these indicators can be used, for example, to determine whether an intervention should be executed on the road sections in the upcoming year, or to predict the future condition of the road sections. Unfortunately, a common problem when working with these data is that there are numerous road sections where no information is available. This can happen either due to errors made during the inspection campaigns themselves or due to using multiple independent sets of geographical information system (GIS) indexed data, when the sets are recorded as noncoincidental GIS shapes. It is of interest to the road manager to estimate the values of the missing condition indicators as best as possible. In this paper, an investigation of the ability to estimate values of road section indicators based on their spatial correlation is presented. The investigation is done by estimating the values of condition indicators for surface defects, and longitudinal and transversal unevenness exploiting the spatial correlation between them, on the Swiss national highway network. It is shown that the values of road section indicators can be estimated based on their spatial correlation with reasonably high levels of accuracy. The variation of the predictive ability per condition indicator is shown.","tags":["PMS","Krigging","Asset Management"],"title":"Investigation of the Ability to Estimate Values of Road Section Condition Indicators Based on Their Spatial Correlation","type":"publication"},{"authors":["Nam Lethanh and Jurgen Hackl and Bryan T. Adey"],"categories":[],"content":"","date":1466899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466899200,"objectID":"b123b518865a3393cdcbb86fed20f133","permalink":"/publication/lethanh2016f/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/publication/lethanh2016f/","section":"publication","summary":"","tags":["Regression Analysis","Markov","Asset Management","Crack","Corrosion","BMS"],"title":"Estimating Markov Transition Probabilities for Reinforced Concrete Bridges based on Mechanistic-Empirical Corrosion Models","type":"publication"},{"authors":[],"categories":["Project Management","Feasibility Study"],"content":"The Project Objectives Responsibilities Lesson Learned Authorship ","date":1461035302,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461035302,"objectID":"89f7c19e0a56791915b34d18decd0876","permalink":"/project/2016-kepco/","publishdate":"2016-04-19T11:08:22+08:00","relpermalink":"/project/2016-kepco/","section":"project","summary":"Feasibility Study for Pangasinan 900MW CFBC Power Station","tags":["Project Management","Feasibility Study","Financial Model","Power Stations"],"title":"900MW Power Station - KEPCO E\u0026C","type":"project"},{"authors":["Nam Lethanh and Kiyoshi Kobayashi and Kiyoyuki Kaito"],"categories":[],"content":"","date":1411104032,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1411104032,"objectID":"2b6cc9ac4a65d38735ba671728675e80","permalink":"/publication/lethanh2014d/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/publication/lethanh2014d/","section":"publication","summary":"The deterioration of a pavement surface can be described in terms of the presence and severity of distinct distresses, like potholes, cracking, and rutting. Each deterioration process is ordinarily described by a set of pavement indicators (e.g., number of potholes, percentage of cracks, international roughness index) that are measured during monitoring and inspection activities. Manifestly, there exist statistical correlations among the deterioration processes. For instance, cracks appearing on a road section may contribute to an increase in pothole occurrence, and vice versa. In order to mathematically formulate the statistical interdependency among deterioration processes, a Poisson hidden Markov model is proposed in this paper. The model describes the complex process of pavement deterioration, which includes the frequent occurrence of local damage (e.g., potholes) as well as the degradation of other pavement indicators (e.g., cracks, roughness). To model the concurrent frequency of local damage, a stochastic Poisson process is used. At the same time, a Markov chain model is used to depict the degradation of other pavement indicators. A numerical estimation approach using Bayesian statistics with a Markov chain Monte Carlo simulation is developed to derive the values of the model’s parameters based on historical information. The applicability of the model was demonstrated through an empirical example using data from a Japanese highway road link..","tags":["Markov","Bayesian","MCMC","Poisson Process","PMS","Asset Management"],"title":"Infrastructure Deterioration Prediction with a Poisson Hidden Markov Model on Time Series Data","type":"publication"},{"authors":["Nam Lethanh and Bryan T. Adey and Dilum N. Fernando"],"categories":[],"content":"","date":1395360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1395360000,"objectID":"6da4c28ecefac3a67567fe167573f72b","permalink":"/publication/lethanh2014/","publishdate":"2014-03-21T00:00:00Z","relpermalink":"/publication/lethanh2014/","section":"publication","summary":"In the existing infrastructure management systems, optimal interventions strategies (OISs) are determined for objects that deteriorate gradually (manifest deterioration process, MDPs), under the assumption that with appropriate inspection and intervention strategies the probability of failure of object can be neglected. Objects that deteriorate suddenly (latent deterioration process, LDPs), for example, due to scouring during a flood or earth movements during an earthquake are not considered. The determination of OISs for an object that deteriorates due to both MDPs and LDPs requires the consideration of both. The latter, however, means that the probability of failure of the object must be considered. In this article, a Markov model is presented that can be used to determine OISs for multiple objects of multiple types affected by uncorrelated MDPs and LDPs. The model is an extension of the model proposed by Mayet and Madanat (Incorporation of seismic considerations in bridge management systems. Computer-Aided Civil and Infrastructure Engineering, 17:185–193, 2002). In the model, a set of condition states (CSs) is used to describe the condition of objects of each type, where each set is composed of non-failure CSs and failure CSs. The probabilities of going from each non-failure CS to each failure CS are estimated using normalised fragility curves, and the probabilities of going from each non-failure CS to each non-failure CS are initially estimated using the Markov deterioration prediction model of Kobayashi, Kaito, and Lethanh (A Bayesian estimation method to improve deterioration prediction for infrastructure system with Markov chain model. International Journal of Architecture, Engineering and Construction, 1:1–13, 2012a) and later adjusted taking into consideration the probabilities of entering the failure CSs. The use of the model is demonstrated using a road link comprising one road section and one bridge.","tags":["Markov","Asset Management"],"title":"Optimal intervention strategies for multiple objects affected by manifest and latent deterioration processes","type":"publication"},{"authors":["Nam Lethanh and Bryan T. Adey"],"categories":[],"content":"","date":1358812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358812800,"objectID":"07bcc46ca117c67a35f841c7d66f2378","permalink":"/publication/lethanh2013a/","publishdate":"2013-01-22T00:00:00Z","relpermalink":"/publication/lethanh2013a/","section":"publication","summary":"In this paper, a probabilistic model for the determination of optimal intervention strategies (OISs) for a road link composed of multiple objects that are affected by gradual deterioration processes is investigated. The model is composed of a deterioration part and a strategy evaluation part. In the deterioration part, a Weibull hazard function is used to represent the deterioration of the individual objects, where the values of the model parameters are to be estimated using inspection data. A threshold condition state (CS) for each object is defined, at which an intervention must be executed. The results of the deterioration part are used as inputs in the strategy evaluation part, in which OISs for individual objects and for the link as a whole are determined. The determination of the optimal strategies takes into consideration impacts on multiple stakeholders. The model is demonstrated by determining the OISs for a fictive road link composed of one bridge and two road sections. The main strengths of the methodology are that past deterioration is taken into consideration and that it is possible to consider the execution of interventions simultaneously and, therefore, associated reductions in impacts that normally occur when interventions are grouped. The main weakness of the methodology is that the condition of the objects is represented using only two CSs, i.e. fully operational and not fully operational.","tags":["Weibull","BMS","PMS","Asset Management"],"title":"Investigation of the Use of a Weibull Model for the Deterioration of Optimal Road Link Intervention Strategies","type":"publication"},{"authors":["Nam Lethanh and Bryan T. Adey"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"4e531a8e3ae79617303ed317eed89ed1","permalink":"/publication/lethanh2013b/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/lethanh2013b/","section":"publication","summary":"In this paper, the potential of using an exponential hidden Markov model to model an indicator of pavement condition as a hidden pavement deterioration process, i.e. one that is not directly measurable, is investigated. It is assumed that the evolution of the values of the pavement condition indices can be adequately described using discrete condition states and modelled as a Markov process. It is also assumed that the values of the indices can be measured over time and represented continuously using exponential distributions. The potential advantage of using such a model is illustrated using a real-world example.","tags":["PMS","Markov","Asset Management"],"title":"Use of Exponential Hidden Markov Models for Modeling Pavement Deterioration","type":"publication"},{"authors":["Kiyoshi Kobayashi and Kiyoyuki Kaito and **Nam Lethanh**"],"categories":[],"content":"","date":1337404832,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1337404832,"objectID":"5f497913beb4ff370360754d7306a3d9","permalink":"/publication/kobayashi2012a/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/publication/kobayashi2012a/","section":"publication","summary":"The application of Markov models as deterioration-forecasting tools has been widely documented in the practice of infrastructure management. The Markov chain models employ monitoring data from visual inspection activities over a period of time in order to predict the deterioration progress of infrastructure systems. Monitoring data play a vital part in the managerial framework of infrastructure management. As a matter of course, the accuracy of deterioration prediction and life cycle cost analysis largely depends on the soundness of monitoring data. However, in reality, monitoring data often contain measurement errors and selection biases, which tend to weaken the correctness of estimation results. In this paper, the authors present a hidden Markov model to tackle selection biases in monitoring data. Selection biases are assumed as random variables. Bayesian estimation and Markov Chain Monte Carlo simulation are employed as techniques in tackling the posterior probability distribution, the random generation of condition states, and the model’s parameters. An empirical application to the Japanese national road system is presented to demonstrate the applicability of the model. Estimation results highlight the fact that the properties of the Markov transition matrix have greatly improved in comparison with the properties obtained from applying the conventional multi-stage exponential Markov model.","tags":["Markov","Bayesian","MCMC","Gibbs Sampling","BMS","PMS","Asset Management"],"title":"A statistical deterioration forecasting method using hidden Markov model for infrastructure management","type":"publication"},{"authors":["Kiyoshi Kobayashi and Kiyoyuki Kaito and **Nam Lethanh**"],"categories":[],"content":"","date":1332134432,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1332134432,"objectID":"c216cb8eb5c36b369c59b327718ce4d3","permalink":"/publication/kobayashi2012/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/publication/kobayashi2012/","section":"publication","summary":"In many practices of bridge asset management, life cycle costs are estimated by statistical de-terioration prediction models based upon monitoring data collected through inspection activities.  In many applications, it is, however, often the case that the validity of statistical deterioration prediction models is flawed by an inadequate stock of inspection dates. In this paper, a systematic methodology is presented to provide estimates of the deterioration process for bridge managers based upon empirical judgments at early stages by experts, and whereby revisions may be made as new data are obtained through later inspections. More concretely, Bayesian estimation methodology is developed to improve the estimation of Markov transition probability of the multi-stage exponential Markov model by Markov chain Monte Carlo method using Gibbs sampling. The paper concludes with an empirical example,  using the real world monitoring data, to demonstrate the applicability of the model and its Bayesian estimation method in the case of incomplete monitoring data.","tags":["Markov","BMS","Bayesian","MCMC","Gibbs Sampling","Asset Management"],"title":"A Bayesian Estimation Method to Improve Deterioration Prediction for Infrastructure System with Markov Chain Model","type":"publication"},{"authors":["Kiyoshi Kobayashi and Kiyoyuki Kaito and **Nam Lethanh**"],"categories":[],"content":"","date":1273449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1273449600,"objectID":"1b1ab327263a7a50eb33e34da5b4d3d5","permalink":"/publication/kobayashi2010a/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/publication/kobayashi2010a/","section":"publication","summary":"In this paper, a time-dependent deterioration forecasting model is presented. In the model the deterioration process is described by transition probabilities, which are conditional upon actual in-service duration. The model is formulated by the multistage Weibull hazard model defined by using multiple Weibull hazard functions. The model can be estimated based upon inspection data that are obtained at discrete points in time. The applicability of the model and the estimation methodology presented in this paper are investigated against an empirical data set of highway utilities in the real world.","tags":["Weibull","Tunnel","Asset Management"],"title":"Deterioration Forecasting Model with Multistage Weibull Hazard Functions","type":"publication"},{"authors":["Nam Lethanh and Thao Nguyendinh and Kiyoyukia Kaito and Kiyoshi Kobayashi"],"categories":[],"content":"","date":1252540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1252540800,"objectID":"6b46a07f008aad49108a04954b0995a3","permalink":"/publication/lethanh2009/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/publication/lethanh2009/","section":"publication","summary":"","tags":["Markov","Asset Management","Benchmarking","PMS"],"title":"A Benchmarking Approach to Pavement Management: Lessons from Vietnam","type":"publication"},{"authors":["Nam Lethanh"],"categories":[],"content":"","date":1252540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1252540800,"objectID":"06b331e10472d650c571bf0aa06e53c4","permalink":"/publication/lethanh2009c/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/publication/lethanh2009c/","section":"publication","summary":"","tags":["Optimization","Markov","Weibull","PMS","BMS","Tunnel","Water Ultilities","Asset Management"],"title":"Stochastic Optimization Methods for Infrastructure Management with Incomplete Monitoring Data","type":"publication"},{"authors":["Nam Lethanh and Kengo Obama and Koyoshi Kobayashi"],"categories":[],"content":"","date":1213056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1213056000,"objectID":"a035b0c76f184310033b05d7b6b3aba3","permalink":"/publication/lethanh2008c/","publishdate":"2008-06-10T00:00:00Z","relpermalink":"/publication/lethanh2008c/","section":"publication","summary":"One of the core techniques for the Pavement Management Systems (PMS) is the deterioration hazard model. The hazard models stochastically forecast the deterioration progresses of pavement based on its historical performance indicators over time. This paper introduces a class of hazard models, which consider the heterogeneity factors by a local mixing mechanism. Special attention is paid to the formulation of transition probability of condition state, which is subjected to follow Markov chain. The paper further introduces optimization approach for selecting best repair strategy based on discounted life cycle cost analysis. The application of the model targets to minimize exposing risks to road administrators. The usefulness and practicability of the model are examined through life cycle costs evaluation with real data of Vietnamese highway system.","tags":["Markov","Asset Management"],"title":"Local mixtures hazard model: A semi-parametric approach to risk management in pavement system","type":"publication"},{"authors":null,"categories":null,"content":" Technical Due Dilligence (TDD) Plant Audit and Asset Management Reliability Study  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"91c1acff572bcfb19666cb6fb670eef4","permalink":"/business/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/business/","section":"","summary":" Technical Due Dilligence (TDD) Plant Audit and Asset Management Reliability Study  ","tags":null,"title":"Business and Services","type":"page"},{"authors":null,"categories":null,"content":" R Graph Gallery Bookdown A Grammar of Data Munipulation (dplyr) Plotly R Open Source Graphing Library The R Graph Gallery rpivotTable  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0d9ca5542a442d81abaed094383e30a0","permalink":"/library/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/library/","section":"","summary":"Collection of References and News","tags":null,"title":"Library","type":"page"},{"authors":["Nam Le"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f8634ce76ae578302c37efda0772e8b1","permalink":"/apps/weibulllcc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/apps/weibulllcc/","section":"apps","summary":"Determination of Optimal Intervention Strategy for Pumps based on Weibull Analysis","tags":null,"title":"Weibull-Analysis-Pumps","type":"apps"}]